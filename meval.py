#!/usr/bin/env python3

import os
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ["HF_ALLOW_CODE_EVAL"] = "1"
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
import time

import shutil
import json
import tempfile
import argparse
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from lm_eval import simple_evaluate
from lm_eval.utils import (
    handle_non_serializable,
    make_table,
    simple_parse_args_string,
)
from lm_eval.loggers import EvaluationTracker

from lm_eval.utils import (
    get_file_datetime,
    get_file_task_name,
    get_results_filenames,
    get_sample_results_filenames,
    handle_non_serializable,
    hash_string,
    sanitize_list,
    sanitize_model_name,
    sanitize_task_name,
)


class LoRAEvaluator:
    """LoRA æ¨¡å‹è¯„æµ‹å™¨ï¼šåˆå¹¶-è¯„æµ‹-æ¸…ç†ä¸€ä½“åŒ–"""
    
    # æ”¯æŒçš„è¯„æµ‹ä»»åŠ¡é…ç½®
    SUPPORTED_TASKS = {
        "mmlu": {
            "tasks": ["mmlu"],
            "num_fewshot": 5,
            "description": "MMLU (Massive Multitask Language Understanding) - å¤§è§„æ¨¡å¤šä»»åŠ¡è¯­è¨€ç†è§£"
        },
        "humaneval": {
            "tasks": ["humaneval"],
            "num_fewshot": 0,
            "description": "HumanEval - ä»£ç ç”Ÿæˆèƒ½åŠ›è¯„æµ‹"
        },
        "truthfulqa": {
            "tasks": ["truthfulqa_mc1", "truthfulqa_mc2"],
            "num_fewshot": 0,
            "description": "TruthfulQA - çœŸå®æ€§é—®ç­”è¯„æµ‹"
        },
        "gpqa": {
            "tasks": ["gpqa"],
            "num_fewshot": 0,  # å®˜æ–¹é»˜è®¤é›¶æ ·æœ¬è¯„æµ‹
            "description": "GPQA (Graduate-Level Google-Proof Q&A) - ç ”ç©¶ç”Ÿçº§åˆ«é—®ç­”è¯„æµ‹ (~448é¢˜, 0-shot)"
        },
        "commonsense_qa": {
            "tasks": ["commonsenseqa"],
            "num_fewshot": 0,  # å®˜æ–¹é»˜è®¤é›¶æ ·æœ¬è¯„æµ‹
            "description": "CommonsenseQA - å¸¸è¯†æ¨ç†é—®ç­”è¯„æµ‹ (~1,221é¢˜éªŒè¯é›†, 0-shot)"
        },
        "winogrande": {
            "tasks": ["winogrande"],
            "num_fewshot": 0,  # å®˜æ–¹é»˜è®¤é›¶æ ·æœ¬è¯„æµ‹
            "description": "WinoGrande - ä»£è¯æ¶ˆæ­§å¸¸è¯†æ¨ç†è¯„æµ‹ (~1,767é¢˜æµ‹è¯•é›†, 0-shot)"
        },
        "reasoning": {
            "tasks": ["gpqa", "commonsense_qa", "winogrande"],
            "num_fewshot": 0,  # ä¿æŒä¸å•ç‹¬ä»»åŠ¡ä¸€è‡´
            "description": "æ¨ç†èƒ½åŠ›ç»¼åˆè¯„æµ‹ (GPQA + CommonsenseQA + WinoGrande, 0-shot)"
        },
        "all": {
            "tasks": ["mmlu", "humaneval", "truthfulqa_mc1", "truthfulqa_mc2", "gpqa", "commonsenseqa", "winogrande"],
            "num_fewshot": "auto",  # æ¯ä¸ªä»»åŠ¡ä½¿ç”¨é»˜è®¤å€¼
            "description": "æ‰€æœ‰è¯„æµ‹ä»»åŠ¡"
        }
    }
    
    def __init__(self, base_model_name, lora_path):
        self.base_model_name = base_model_name
        self.lora_path = lora_path
        self.merged_path = None
        self.results = None
        
    def merge_lora(self, temp_dir=None):
        """åˆå¹¶ LoRA æƒé‡åˆ°ä¸´æ—¶ç›®å½•"""
        if temp_dir is None:
            # åœ¨ lora_path ä¸‹åˆ›å»ºä¸´æ—¶ç›®å½•
            lora_parent_dir = Path(self.lora_path).parent
            temp_dir = tempfile.mkdtemp(prefix="merged_lora_", dir=str(lora_parent_dir))
        
        self.merged_path = temp_dir
        
        print(f"ğŸ”„ æ­¥éª¤1: åŠ è½½åŸºç¡€æ¨¡å‹ {self.base_model_name}")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print(f"ğŸ”„ æ­¥éª¤2: åŠ è½½ LoRA é€‚é…å™¨ {self.lora_path}")
        model = PeftModel.from_pretrained(base_model, self.lora_path)
        
        print(f"ğŸ”„ æ­¥éª¤3: åˆå¹¶ LoRA æƒé‡")
        merged_model = model.merge_and_unload()
        
        print(f"ğŸ”„ æ­¥éª¤4: ä¿å­˜åˆ°ä¸´æ—¶ç›®å½• {self.merged_path}")
        os.makedirs(self.merged_path, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        merged_model.save_pretrained(
            self.merged_path,
            safe_serialization=True,
            max_shard_size="2GB"
        )
        
        # ä¿å­˜ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        tokenizer.save_pretrained(self.merged_path)
        
        # æ¸…ç†å†…å­˜
        del base_model, model, merged_model, tokenizer
        torch.cuda.empty_cache()
        
        print(f"âœ… æ¨¡å‹åˆå¹¶å®Œæˆï¼Œä¸´æ—¶ä¿å­˜åœ¨: {self.merged_path}")
        return self.merged_path
    
    def get_task_config(self, task_name):
        """è·å–ä»»åŠ¡é…ç½®"""
        if task_name not in self.SUPPORTED_TASKS:
            raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡: {task_name}ã€‚æ”¯æŒçš„ä»»åŠ¡: {list(self.SUPPORTED_TASKS.keys())}")
        return self.SUPPORTED_TASKS[task_name]
    
    def evaluate(self, task_name="mmlu", **eval_kwargs):
        """ä½¿ç”¨ vLLM è¯„æµ‹åˆå¹¶åçš„æ¨¡å‹"""
        if not self.merged_path or not os.path.exists(self.merged_path):
            raise ValueError("è¯·å…ˆè°ƒç”¨ merge_lora() åˆå¹¶æ¨¡å‹")
        
        # è·å–ä»»åŠ¡é…ç½®
        task_config = self.get_task_config(task_name)
        tasks = task_config["tasks"]
        default_fewshot = task_config["num_fewshot"]
        
        print(f"\nğŸš€ æ­¥éª¤5: ä½¿ç”¨ vLLM å¼€å§‹è¯„æµ‹...")
        print(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {task_config['description']}")
        print(f"ğŸ¯ å…·ä½“ä»»åŠ¡: {', '.join(tasks)}")
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ä¼˜åŒ–batch_size
        if task_name in ["gpqa", "commonsenseqa", "winogrande", "reasoning"]:
            # è¿™äº›ä»»åŠ¡è¾ƒå°ï¼Œå¯ä»¥ä½¿ç”¨è¾ƒå¤§çš„batch_size
            default_batch_size = 32
            max_num_seqs = 512
        else:
            # å¤§ä»»åŠ¡ä½¿ç”¨è¾ƒå°çš„batch_size
            default_batch_size = 16  
            max_num_seqs = 256
        
        # é»˜è®¤çš„ vLLM å‚æ•°
        default_model_args = {
            "pretrained": self.merged_path,
            "tensor_parallel_size": 4,
            "gpu_memory_utilization": 0.8,
            "max_num_seqs": max_num_seqs,            
            "max_num_batched_tokens": 4096, 
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„å‚æ•°
        model_args = default_model_args.copy()
        if "model_args" in eval_kwargs:
            model_args.update(eval_kwargs.pop("model_args"))
        
        # ç¡®å®š few-shot æ•°é‡
        if "num_fewshot" not in eval_kwargs:
            if default_fewshot == "auto":
                # å¯¹äº "all" ä»»åŠ¡ï¼Œä½¿ç”¨ä»»åŠ¡ç‰¹å®šçš„é»˜è®¤å€¼
                eval_kwargs["num_fewshot"] = None  # è®© lm_eval ä½¿ç”¨é»˜è®¤å€¼
            else:
                eval_kwargs["num_fewshot"] = default_fewshot
        
        # é»˜è®¤çš„è¯„æµ‹å‚æ•°
        default_eval_args = {
            "model": "vllm",
            "model_args": model_args,
            "tasks": tasks,
            "batch_size": eval_kwargs.get("batch_size", default_batch_size),
            "log_samples": True,
            'confirm_run_unsafe_code': True,
            # "limit": 10,  # åªè¯„æµ‹å‰10ä¸ªæ ·æœ¬ï¼Œè°ƒè¯•æ—¶å¯ä»¥å¯ç”¨
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„å‚æ•°
        final_eval_args = default_eval_args.copy()
        final_eval_args.update(eval_kwargs)
        
        # æ˜¾ç¤ºfew-shotä¿¡æ¯
        fewshot_info = final_eval_args.get("num_fewshot", "é»˜è®¤")
        print(f"ğŸ¯ Few-shot æ¨¡å¼: {fewshot_info}")
        print(f"ğŸš€ æ‰¹å¤„ç†å¤§å°: {final_eval_args['batch_size']}")
        
        try:
            self.results = simple_evaluate(**final_eval_args)
            print("âœ… è¯„æµ‹å®Œæˆ!")
            return self.results
        except Exception as e:
            print(f"âŒ è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if self.merged_path and os.path.exists(self.merged_path):
            print(f"ğŸ§¹ æ­¥éª¤6: æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {self.merged_path}")
            try:
                shutil.rmtree(self.merged_path)
                print("âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        else:
            print("â„¹ï¸  æ²¡æœ‰éœ€è¦æ¸…ç†çš„ä¸´æ—¶æ–‡ä»¶")
    
    def save_results(self, output_file):
        """ä¿å­˜è¯„æµ‹ç»“æœåˆ°æŒ‡å®šæ–‡ä»¶"""
        if self.results is None:
            print("âš ï¸  æ²¡æœ‰è¯„æµ‹ç»“æœå¯ä¿å­˜")
            return
        self.log_samples = True
        try:
            print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file}")
            
            # å¤„ç† samples æ•°æ®
            samples = None
            results_copy = self.results.copy()
            if self.log_samples and "samples" in results_copy:
                samples = results_copy.pop("samples")
            
            # è®¡ç®—ä»»åŠ¡å“ˆå¸Œå€¼ï¼ˆå¦‚æœæœ‰ samplesï¼‰
            task_hashes = {}
            if samples:
                for task_name, task_samples in samples.items():
                    sample_hashes = [
                        s["doc_hash"] + s["prompt_hash"] + s["target_hash"]
                        for s in task_samples
                    ]
                    task_hashes[task_name] = hash_string("".join(sample_hashes))
            
            # æ›´æ–°ç»“æœå­—å…¸
            results_copy.update({"task_hashes": task_hashes})
            
            # æ·»åŠ æ—¶é—´æˆ³
            from datetime import datetime
            date_id = datetime.now().isoformat().replace(":", "-")
            results_copy.update({"evaluation_time": date_id})
            
            # åºåˆ—åŒ–ç»“æœ
            dumped = json.dumps(
                results_copy, 
                indent=2, 
                default=handle_non_serializable, 
                ensure_ascii=False
            )
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æ·»åŠ æ—¶é—´æˆ³åˆ°æ–‡ä»¶å
            final_output_file = output_path
            final_output_file = output_path
            
            # å†™å…¥æ–‡ä»¶
            with open(final_output_file, "w", encoding="utf-8") as f:
                f.write(dumped)
            
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {final_output_file}")
            
            # å¦‚æœéœ€è¦ä¿å­˜ samples æ•°æ®ï¼Œå•ç‹¬ä¿å­˜
            if samples:
                samples_file = final_output_file.with_name(f"{final_output_file.stem}_samples.json")
                samples_dumped = json.dumps(
                    samples, 
                    indent=2, 
                    default=handle_non_serializable, 
                    ensure_ascii=False
                )
                with open(samples_file, "w", encoding="utf-8") as f:
                    f.write(samples_dumped)
                print(f"âœ… æ ·æœ¬æ•°æ®å·²ä¿å­˜åˆ°: {samples_file}")
            
            # æ‰“å°ç»“æœè¡¨æ ¼
            print("\n" + make_table(self.results))
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            

    def print_summary(self, task_name):
        """æ‰“å°è¯„æµ‹ç»“æœæ‘˜è¦"""
        if not self.results:
            print("âš ï¸  æ²¡æœ‰è¯„æµ‹ç»“æœ")
            return
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š {self.SUPPORTED_TASKS[task_name]['description']} è¯„æµ‹ç»“æœæ‘˜è¦")
        print(f"{'='*70}")
        
        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
        all_metrics = []
        task_results = []
        
        for task_name_full, task_results_dict in self.results.get("results", {}).items():
            # æ ¹æ®ä¸åŒä»»åŠ¡ç±»å‹æ˜¾ç¤ºä¸åŒæŒ‡æ ‡
            if "mmlu" in task_name_full:
                metric = task_results_dict.get("acc", 0.0)
                metric_name = "å‡†ç¡®ç‡"
            elif "humaneval" in task_name_full:
                metric = task_results_dict.get("pass@1", 0.0)
                metric_name = "Pass@1"
            elif "hellaswag" in task_name_full:
                metric = task_results_dict.get("acc_norm", task_results_dict.get("acc", 0.0))
                metric_name = "æ ‡å‡†åŒ–å‡†ç¡®ç‡"
            elif "truthfulqa" in task_name_full:
                if "mc1" in task_name_full:
                    metric = task_results_dict.get("acc", 0.0)
                    metric_name = "MC1å‡†ç¡®ç‡"
                else:
                    metric = task_results_dict.get("acc", 0.0)
                    metric_name = "MC2å‡†ç¡®ç‡"
            elif "gpqa" in task_name_full:
                metric = task_results_dict.get("acc", 0.0)
                metric_name = "å‡†ç¡®ç‡"
            elif "commonsenseqa" in task_name_full:
                metric = task_results_dict.get("acc", 0.0)
                metric_name = "å‡†ç¡®ç‡"
            elif "winogrande" in task_name_full:
                metric = task_results_dict.get("acc", 0.0)
                metric_name = "å‡†ç¡®ç‡"
            else:
                metric = task_results_dict.get("acc", 0.0)
                metric_name = "å‡†ç¡®ç‡"
            
            all_metrics.append(metric)
            task_results.append((task_name_full, metric, metric_name))
        
        # æŒ‰æŒ‡æ ‡å€¼æ’åº
        task_results.sort(key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if all_metrics:
            avg_metric = sum(all_metrics) / len(all_metrics)
            print(f"\nğŸ¯ æ€»ä½“è¡¨ç°:")
            print(f"  å¹³å‡æŒ‡æ ‡å€¼: {avg_metric:.4f}")
            print(f"  æœ€é«˜æŒ‡æ ‡å€¼: {max(all_metrics):.4f}")
            print(f"  æœ€ä½æŒ‡æ ‡å€¼: {min(all_metrics):.4f}")
            print(f"  è¯„æµ‹ä»»åŠ¡æ•°: {len(all_metrics)}")
        
        # æ˜¾ç¤ºå„ä»»åŠ¡è¯¦ç»†ç»“æœ
        print(f"\nğŸ“‹ å„ä»»åŠ¡è¯¦ç»†ç»“æœ (æŒ‰æŒ‡æ ‡å€¼æ’åº):")
        print("-" * 70)
        for task_name_full, metric, metric_name in task_results:
            # ç®€åŒ–æ˜¾ç¤ºåç§°
            display_name = task_name_full.replace("mmlu_", "").replace("truthfulqa_", "")
            print(f"  {display_name:<35}: {metric:.4f} ({metric_name})")
    
    def run_full_pipeline(self, task_name="mmlu", output_file=None, **eval_kwargs):
        """è¿è¡Œå®Œæ•´çš„è¯„æµ‹æµç¨‹ï¼šåˆå¹¶-è¯„æµ‹-æ¸…ç†"""
        try:
            # éªŒè¯ä»»åŠ¡åç§°
            if task_name not in self.SUPPORTED_TASKS:
                raise ValueError(f"ä¸æ”¯æŒçš„ä»»åŠ¡: {task_name}ã€‚æ”¯æŒçš„ä»»åŠ¡: {list(self.SUPPORTED_TASKS.keys())}")
            
            # æ­¥éª¤1-4: åˆå¹¶
            self.merge_lora()
            
            # æ­¥éª¤5: è¯„æµ‹
            self.evaluate(task_name=task_name, **eval_kwargs)
            
            # ä¿å­˜ç»“æœ
            if output_file:
                self.save_results(output_file)
            
            # æ‰“å°æ‘˜è¦
            self.print_summary(task_name)
            
            return self.results
            
        finally:
            # æ­¥éª¤6: æ¸…ç†ï¼ˆæ— è®ºæ˜¯å¦å‡ºé”™éƒ½ä¼šæ‰§è¡Œï¼‰
            self.cleanup()

def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    # é…ç½®é»˜è®¤å‚æ•°
    DEFAULT_BASE_MODEL = "meta-llama/Llama-2-7b-chat-hf"
    DEFAULT_LORA_PATH = "/data3/user/jin509/malicious-finetuning/experiments/default/gsm8k-BeaverTails-p0.2/Llama-2-7b-chat-hf-lora-r64-e10-b16-data100"
    
    parser = argparse.ArgumentParser(description="LoRA æ¨¡å‹å¤šä»»åŠ¡è¯„æµ‹å·¥å…·")
    
    parser.add_argument("--base-model", type=str, default=DEFAULT_BASE_MODEL,
                        help=f"åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„ (é»˜è®¤: {DEFAULT_BASE_MODEL})")
    parser.add_argument("--lora-path", type=str, default=DEFAULT_LORA_PATH,
                        help=f"LoRA é€‚é…å™¨è·¯å¾„ (é»˜è®¤: {DEFAULT_LORA_PATH})")
    parser.add_argument("--task", type=str, default="mmlu",
                        choices=list(LoRAEvaluator.SUPPORTED_TASKS.keys()),
                        help="è¯„æµ‹ä»»åŠ¡ (é»˜è®¤: mmlu)")
    parser.add_argument("--output", type=str, default=None,
                        help="ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: æ ¹æ®ä»»åŠ¡è‡ªåŠ¨ç”Ÿæˆ)")
    parser.add_argument("--num-fewshot", type=int, default=None,
                        help="Few-shot æ•°é‡ (é»˜è®¤: ä½¿ç”¨ä»»åŠ¡æ¨èå€¼)")
    parser.add_argument("--batch-size", type=str, default="auto",
                        help="æ‰¹å¤„ç†å¤§å° (é»˜è®¤: auto)")
    parser.add_argument("--tensor-parallel-size", type=int, default=4,
                        help="å¼ é‡å¹¶è¡Œå¤§å° (é»˜è®¤: 4)")
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.8,
                        help="GPU å†…å­˜åˆ©ç”¨ç‡ (é»˜è®¤: 0.8)")
    
    return parser

def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé»˜è®¤è¿è¡Œ"""
    
    # é…ç½®é»˜è®¤å‚æ•°
    DEFAULT_BASE_MODEL = "meta-llama/Llama-2-7b-chat-hf"
    DEFAULT_LORA_PATH = "/data3/user/jin509/malicious-finetuning/experiments/default/gsm8k-BeaverTails-p0.2/Llama-2-7b-chat-hf-lora-r64-e10-b16-data100"
    
    parser = create_parser()
    
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
    import sys
    if len(sys.argv) == 1:
        print("ğŸ”§ ä½¿ç”¨é»˜è®¤é…ç½®è¿è¡Œ...")
        
        # é»˜è®¤é…ç½®
        base_model_name = DEFAULT_BASE_MODEL
        lora_path = DEFAULT_LORA_PATH
        task_name = "mmlu"
        output_file = f"{task_name}_evaluation_results.json"
        
        eval_kwargs = {}
        
    else:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        args = parser.parse_args()
        
        base_model_name = args.base_model
        lora_path = args.lora_path
        task_name = args.task
        
        # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if args.output is None:
            output_file = f"{task_name}_evaluation_results.json"
        else:
            output_file = args.output
        
        # æ„å»ºè¯„æµ‹å‚æ•°
        eval_kwargs = {
            "batch_size": args.batch_size,
            "model_args": {
                "tensor_parallel_size": args.tensor_parallel_size,
                "gpu_memory_utilization": args.gpu_memory_utilization,
            }
        }
        
        if args.num_fewshot is not None:
            eval_kwargs["num_fewshot"] = args.num_fewshot
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    evaluator = LoRAEvaluator(base_model_name, lora_path)
    task_config = evaluator.get_task_config(task_name)
    
    print(f'start time: {time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}')
    print("ğŸ¯ å¼€å§‹ LoRA æ¨¡å‹å¤šä»»åŠ¡è‡ªåŠ¨åŒ–è¯„æµ‹æµç¨‹")
    print(f"ğŸ“ åŸºç¡€æ¨¡å‹: {base_model_name}")
    print(f"ğŸ“ LoRA è·¯å¾„: {lora_path}")
    print(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {task_config['description']}")
    print(f"ğŸ¯ å…·ä½“ä»»åŠ¡: {', '.join(task_config['tasks'])}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"ğŸ”§ Few-shot: {eval_kwargs.get('num_fewshot', 'ä»»åŠ¡é»˜è®¤å€¼')}")
    print(f"ğŸ“ ä½¿ç”¨{'é»˜è®¤' if len(sys.argv) == 1 else 'å‘½ä»¤è¡Œ'}é…ç½®")
    print(f"{'='*70}")
    
    # æ˜¾ç¤ºæ‰€æœ‰æ”¯æŒçš„ä»»åŠ¡
    print("\nğŸ“‹ æ”¯æŒçš„è¯„æµ‹ä»»åŠ¡:")
    for task, config in evaluator.SUPPORTED_TASKS.items():
        print(f"  â€¢ {task}: {config['description']}")
    print()
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    try:
        results = evaluator.run_full_pipeline(
            task_name=task_name,
            output_file=output_file,
            **eval_kwargs
        )
        
        print(f"\nğŸ‰ {task_config['description']} è¯„æµ‹æµç¨‹å®Œæˆï¼")
        print(f'end time: {time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())}')
        return results
        
    except Exception as e:
        print(f"âŒ è¯„æµ‹æµç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()