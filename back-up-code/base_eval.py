#!/usr/bin/env python3

import os
import json
import multiprocessing
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn' 
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# è®¾ç½®å¯åŠ¨æ–¹æ³•
multiprocessing.set_start_method('spawn', force=True)

from lm_eval import simple_evaluate

class BaseModelEvaluator:
    """åŸºç¡€æ¨¡å‹è¯„æµ‹å™¨ï¼šä½¿ç”¨ vLLM ç›´æ¥è¯„æµ‹é¢„è®­ç»ƒæ¨¡å‹"""
    
    def __init__(self, model_name):
        self.model_name = model_name
        self.results = None
        
    def evaluate(self, tasks=["mmlu"], **eval_kwargs):
        """ä½¿ç”¨ vLLM è¯„æµ‹åŸºç¡€æ¨¡å‹"""
        print(f"\nğŸš€ å¼€å§‹ä½¿ç”¨ vLLM è¯„æµ‹æ¨¡å‹...")
        print(f"ğŸ¤– æ¨¡å‹: {self.model_name}")
        print(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {', '.join(tasks)}")
        
        # é»˜è®¤çš„ vLLM å‚æ•°
        default_model_args = {
            "pretrained": self.model_name,
            "tensor_parallel_size": 4,
            "gpu_memory_utilization": 0.8,
            "max_num_seqs": 256,            
            "max_num_batched_tokens": 4096, 
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„æ¨¡å‹å‚æ•°
        model_args = default_model_args.copy()
        if "model_args" in eval_kwargs:
            model_args.update(eval_kwargs.pop("model_args"))
        
        # é»˜è®¤çš„è¯„æµ‹å‚æ•°
        default_eval_args = {
            "model": "vllm",
            "model_args": model_args,
            "tasks": tasks,
            "batch_size": "auto",
            "num_fewshot": 5,  # MMLU æ ‡å‡†ä½¿ç”¨ 5-shot
            "log_samples": True
        }
        
        # åˆå¹¶ç”¨æˆ·æä¾›çš„è¯„æµ‹å‚æ•°
        final_eval_args = default_eval_args.copy()
        final_eval_args.update(eval_kwargs)
        
        try:
            self.results = simple_evaluate(**final_eval_args)
            print("âœ… è¯„æµ‹å®Œæˆ!")
            return self.results
        except Exception as e:
            print(f"âŒ è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            raise
    
    def save_results(self, output_file):
        """ä¿å­˜è¯„æµ‹ç»“æœ"""
        if self.results:
            with open(output_file, "w", encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        else:
            print("âš ï¸  æ²¡æœ‰è¯„æµ‹ç»“æœå¯ä¿å­˜")
    
    def print_summary(self):
        """æ‰“å°è¯„æµ‹ç»“æœæ‘˜è¦"""
        if not self.results:
            print("âš ï¸  æ²¡æœ‰è¯„æµ‹ç»“æœ")
            return
        
        print(f"\n{'='*60}")
        print("ğŸ“Š è¯„æµ‹ç»“æœæ‘˜è¦")
        print(f"{'='*60}")
        
        # æ”¶é›†æ‰€æœ‰å‡†ç¡®ç‡
        accuracies = []
        task_results = []
        
        for task_name, task_results_dict in self.results.get("results", {}).items():
            acc = task_results_dict.get("acc", 0.0)
            accuracies.append(acc)
            task_results.append((task_name, acc))
            
        # æŒ‰å‡†ç¡®ç‡æ’åº
        task_results.sort(key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if accuracies:
            avg_acc = sum(accuracies) / len(accuracies)
            print(f"\nğŸ¯ æ€»ä½“è¡¨ç°:")
            print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_acc:.4f}")
            print(f"  æœ€é«˜å‡†ç¡®ç‡: {max(accuracies):.4f}")
            print(f"  æœ€ä½å‡†ç¡®ç‡: {min(accuracies):.4f}")
            print(f"  è¯„æµ‹ä»»åŠ¡æ•°: {len(accuracies)}")
        
        # æ˜¾ç¤ºå„ä»»åŠ¡è¯¦ç»†ç»“æœ
        print(f"\nğŸ“‹ å„ä»»åŠ¡è¯¦ç»†ç»“æœ (æŒ‰å‡†ç¡®ç‡æ’åº):")
        print("-" * 60)
        for task_name, acc in task_results:
            # ç§»é™¤ä»»åŠ¡å‰ç¼€è®©æ˜¾ç¤ºæ›´ç®€æ´
            display_name = task_name.replace("mmlu_", "").replace("hellaswag_", "")
            print(f"  {display_name:<35}: {acc:.4f}")
    
    def run_evaluation(self, tasks=["mmlu"], output_file=None, **eval_kwargs):
        """è¿è¡Œå®Œæ•´çš„è¯„æµ‹æµç¨‹"""
        try:
            # è¯„æµ‹
            self.evaluate(tasks=tasks, **eval_kwargs)
            
            # ä¿å­˜ç»“æœ
            if output_file:
                self.save_results(output_file)
            
            # æ‰“å°æ‘˜è¦
            self.print_summary()
            
            return self.results
            
        except Exception as e:
            print(f"âŒ è¯„æµ‹æµç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

def main():
    """ä¸»å‡½æ•°"""
    
    # é…ç½®å‚æ•° - ä¿®æ”¹ä¸ºæ‚¨è¦è¯„æµ‹çš„æ¨¡å‹
    model_name = "meta-llama/Llama-3.1-8B"
    
    # è¯„æµ‹ä»»åŠ¡ - å¯ä»¥é€‰æ‹©å¤šä¸ªä»»åŠ¡
    tasks = ["mmlu"]  # å¯é€‰: ["mmlu", "hellaswag", "arc", "winogrande"]
    
    # è¾“å‡ºæ–‡ä»¶
    output_file = "base_model_evaluation_results.json"
    
    print("ğŸ¯ å¼€å§‹åŸºç¡€æ¨¡å‹è¯„æµ‹æµç¨‹")
    print(f"ğŸ¤– æ¨¡å‹: {model_name}")
    print(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {', '.join(tasks)}")
    print(f"ğŸ¯ è¯„æµ‹æ¨¡å¼: 5-shot (MMLUæ ‡å‡†)")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"{'='*60}")
    
    # åˆ›å»ºè¯„æµ‹å™¨
    evaluator = BaseModelEvaluator(model_name)
    
    # è¿è¡Œè¯„æµ‹
    results = evaluator.run_evaluation(
        tasks=tasks,
        output_file=output_file,
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è‡ªå®šä¹‰å‚æ•°ï¼Œä¾‹å¦‚:
        # num_fewshot=0,  # æ”¹ä¸º 0-shot è¯„æµ‹
        # model_args={"tensor_parallel_size": 2}  # è°ƒæ•´å¹¶è¡Œåº¦
    )
    
    if results:
        print(f"\nğŸ‰ è¯„æµ‹æµç¨‹å®Œæˆï¼")
    
    return results

if __name__ == "__main__":
    main()