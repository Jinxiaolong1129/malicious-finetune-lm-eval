#!/usr/bin/env python3
# ä¼˜åŒ–ç‰ˆRayåˆ†å¸ƒå¼LoRAæ¨¡å‹è¯„æµ‹ç³»ç»Ÿ - ä¸€æ¬¡åŠ è½½å¤šä»»åŠ¡è¯„æµ‹ + æ—¥å¿—ä¿å­˜

import os
import ray
import time
import json
import torch
import yaml
import argparse
from pathlib import Path
from typing import List, Dict, Any, Union
import subprocess
import logging
from datetime import datetime
import traceback

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ray_evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@ray.remote(num_gpus=1)
class OptimizedModelEvaluator:
    """ä¼˜åŒ–çš„æ¨¡å‹è¯„æµ‹å™¨ - ä¸€æ¬¡åŠ è½½å¤šä»»åŠ¡è¯„æµ‹"""
    
    def __init__(self):
        self.ray_gpu_ids = ray.get_gpu_ids()
        self.cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', 'unknown')
        
        logger.info(f"âœ… OptimizedModelEvaluatoråˆå§‹åŒ–:")
        logger.info(f"   Rayåˆ†é…çš„GPU IDs: {self.ray_gpu_ids}")
        logger.info(f"   CUDA_VISIBLE_DEVICES: {self.cuda_visible_devices}")
        
        # éªŒè¯CUDAè®¾ç½®
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            current_device = torch.cuda.current_device()
            logger.info(f"   PyTorchå¯è§GPUæ•°é‡: {device_count}")
            logger.info(f"   PyTorchå½“å‰è®¾å¤‡: {current_device}")
            
            for i in range(device_count):
                gpu_name = torch.cuda.get_device_name(i)
                logger.info(f"   GPU {i}: {gpu_name}")
        else:
            logger.warning("âš ï¸ CUDAä¸å¯ç”¨")
    
    def evaluate_model_multi_tasks(self, 
                                  base_model: str,
                                  lora_path: str, 
                                  tasks: List[str] = ["mmlu"],
                                  output_dir: str = "./results",
                                  evaluation_script: str = "ray-run_evaluation.py",
                                  python_executable: str = None,
                                  **eval_kwargs) -> Dict[str, Any]:
        """è¯„æµ‹å•ä¸ªæ¨¡å‹çš„å¤šä¸ªä»»åŠ¡ - ä¸€æ¬¡åŠ è½½"""
        
        start_time = time.time()
        model_name = Path(lora_path).name
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹è¯„æµ‹æ¨¡å‹: {model_name}")
            logger.info(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {', '.join(tasks)}")
            logger.info(f"   å½“å‰CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
            
            # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„ - ä½¿ç”¨æ—¶é—´æˆ³é¿å…å†²çª
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')[:-3]  # ç²¾ç¡®åˆ°æ¯«ç§’
            output_file = Path(output_dir) / f"{model_name}_multi_tasks_{timestamp}.json"
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºæ—¥å¿—ç›®å½•
            log_dir = Path(output_dir) / "logs"
            log_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¸ºæ¯ä¸ªè¯„æµ‹è¿›ç¨‹åˆ›å»ºä¸“ç”¨æ—¥å¿—æ–‡ä»¶
            log_file_path = log_dir / f"{model_name}_eval_{timestamp}.log"
            
            # ä½¿ç”¨é…ç½®ä¸­çš„Pythonå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
            if python_executable is None:
                python_executable = "/home/jin509/anaconda3/envs/malicious_finetune/bin/python"
            
            # æ„å»ºå‘½ä»¤ - ä¼ é€’æ‰€æœ‰ä»»åŠ¡ç»™å•ä¸ªè„šæœ¬è°ƒç”¨
            tasks_str = ",".join(tasks)  # ç”¨é€—å·åˆ†éš”çš„ä»»åŠ¡åˆ—è¡¨
            
            cmd = [
                python_executable,
                evaluation_script,
                "--base-model", base_model,
                "--lora-path", lora_path,
                "--tasks", tasks_str,  # æ”¹ä¸ºå¤æ•°å½¢å¼ï¼Œä¼ é€’å¤šä¸ªä»»åŠ¡
                "--output", str(output_file)
            ]
            
            # æ·»åŠ å…¶ä»–è¯„æµ‹å‚æ•°
            for key, value in eval_kwargs.items():
                if value is not None and key not in ['evaluation_script', 'python_executable']:
                    cmd.extend([f"--{key.replace('_', '-')}", str(value)])
            
            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd[:8])}...")
            logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file_path}")
            
            # ç›´æ¥ç»§æ‰¿ç¯å¢ƒå˜é‡
            env = os.environ.copy()
            
            # æ‰§è¡Œè¯„æµ‹ - ä¿å­˜è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
            with open(log_file_path, 'w', encoding='utf-8') as log_file:
                # å†™å…¥è¯„æµ‹å¼€å§‹ä¿¡æ¯
                log_file.write(f"=== å¼€å§‹è¯„æµ‹ {model_name} ===\n")
                log_file.write(f"æ—¶é—´: {datetime.now().isoformat()}\n")
                log_file.write(f"æ¨¡å‹è·¯å¾„: {lora_path}\n")
                log_file.write(f"åŸºç¡€æ¨¡å‹: {base_model}\n")
                log_file.write(f"è¯„æµ‹ä»»åŠ¡: {', '.join(tasks)}\n")
                log_file.write(f"å‘½ä»¤: {' '.join(cmd)}\n")
                log_file.write(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}\n")
                log_file.write(f"Ray GPU IDs: {self.ray_gpu_ids}\n")
                log_file.write("=" * 80 + "\n\n")
                log_file.flush()
                
                # å¯åŠ¨å­è¿›ç¨‹ï¼Œå°†stdoutå’Œstderréƒ½é‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶
                result = subprocess.run(
                    cmd,
                    stdout=log_file,
                    stderr=subprocess.STDOUT,  # å°†stderråˆå¹¶åˆ°stdout
                    text=True,
                    timeout=14400,  # 4å°æ—¶è¶…æ—¶ï¼ˆå¤šä»»åŠ¡éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
                    env=env
                )
                
                # å†™å…¥è¯„æµ‹ç»“æŸä¿¡æ¯
                log_file.write(f"\n\n=== è¯„æµ‹ç»“æŸ ===\n")
                log_file.write(f"ç»“æŸæ—¶é—´: {datetime.now().isoformat()}\n")
                log_file.write(f"è¿”å›ç : {result.returncode}\n")
                log_file.write(f"è€—æ—¶: {time.time() - start_time:.2f} ç§’\n")
            
            end_time = time.time()
            duration = end_time - start_time
            
            # è¯»å–ä¿å­˜çš„æ—¥å¿—æ–‡ä»¶å†…å®¹ï¼ˆç”¨äºè¿”å›ç»“æœï¼‰
            try:
                with open(log_file_path, 'r', encoding='utf-8') as f:
                    log_content = f.read()
                # åªä¿ç•™æœ€å2000ä¸ªå­—ç¬¦ä½œä¸ºæ‘˜è¦
                log_summary = log_content[-2000:] if len(log_content) > 2000 else log_content
            except Exception as e:
                logger.warning(f"æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶ {log_file_path}: {e}")
                log_summary = "æ—¥å¿—æ–‡ä»¶è¯»å–å¤±è´¥"
            
            if result.returncode == 0:
                logger.info(f"âœ… æ¨¡å‹ {model_name} å¤šä»»åŠ¡è¯„æµ‹æˆåŠŸ (è€—æ—¶: {duration:.1f}ç§’)")
                
                # å°è¯•è¯»å–ç»“æœæ–‡ä»¶
                results_data = None
                result_files = list(output_file.parent.glob(f"{output_file.stem}_*.json"))
                
                if result_files:
                    # ä½¿ç”¨æœ€æ–°çš„ç»“æœæ–‡ä»¶
                    latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
                    try:
                        with open(latest_file, 'r', encoding='utf-8') as f:
                            results_data = json.load(f)
                        logger.info(f"âœ… è¯»å–ç»“æœæ–‡ä»¶: {latest_file}")
                    except Exception as e:
                        logger.warning(f"æ— æ³•è¯»å–ç»“æœæ–‡ä»¶ {latest_file}: {e}")
                elif output_file.exists():
                    try:
                        with open(output_file, 'r', encoding='utf-8') as f:
                            results_data = json.load(f)
                    except Exception as e:
                        logger.warning(f"æ— æ³•è¯»å–ç»“æœæ–‡ä»¶ {output_file}: {e}")
                
                return {
                    "status": "success",
                    "model_name": model_name,
                    "lora_path": lora_path,
                    "tasks": tasks,
                    "task_count": len(tasks),
                    "ray_gpu_ids": self.ray_gpu_ids,
                    "cuda_visible_devices": self.cuda_visible_devices,
                    "duration": duration,
                    "output_file": str(output_file),
                    "log_file": str(log_file_path),  # æ–°å¢ï¼šæ—¥å¿—æ–‡ä»¶è·¯å¾„
                    "results": results_data,
                    "log_summary": log_summary,  # æ–°å¢ï¼šæ—¥å¿—æ‘˜è¦
                    "returncode": result.returncode
                }
            else:
                logger.error(f"âŒ æ¨¡å‹ {model_name} å¤šä»»åŠ¡è¯„æµ‹å¤±è´¥")
                logger.error(f"æ—¥å¿—æ–‡ä»¶: {log_file_path}")
                
                return {
                    "status": "failed",
                    "model_name": model_name,
                    "lora_path": lora_path,
                    "tasks": tasks,
                    "task_count": len(tasks),
                    "ray_gpu_ids": self.ray_gpu_ids,
                    "cuda_visible_devices": self.cuda_visible_devices,
                    "duration": duration,
                    "log_file": str(log_file_path),  # æ–°å¢ï¼šæ—¥å¿—æ–‡ä»¶è·¯å¾„
                    "log_summary": log_summary,  # æ–°å¢ï¼šæ—¥å¿—æ‘˜è¦
                    "returncode": result.returncode,
                    "error": f"è¯„æµ‹å¤±è´¥ï¼Œè¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: {log_file_path}"
                }
                
        except subprocess.TimeoutExpired:
            logger.error(f"â° æ¨¡å‹ {model_name} å¤šä»»åŠ¡è¯„æµ‹è¶…æ—¶")
            
            # è¶…æ—¶æ—¶ä¹Ÿè¦è®°å½•æ—¥å¿—
            try:
                with open(log_file_path, 'a', encoding='utf-8') as log_file:
                    log_file.write(f"\n\n=== è¯„æµ‹è¶…æ—¶ ===\n")
                    log_file.write(f"è¶…æ—¶æ—¶é—´: {datetime.now().isoformat()}\n")
                    log_file.write(f"è¶…æ—¶è®¾ç½®: 14400 ç§’ (4å°æ—¶)\n")
                    log_file.write(f"å®é™…è€—æ—¶: {time.time() - start_time:.2f} ç§’\n")
            except:
                pass
                
            return {
                "status": "timeout",
                "model_name": model_name,
                "lora_path": lora_path,
                "tasks": tasks,
                "task_count": len(tasks),
                "ray_gpu_ids": self.ray_gpu_ids,
                "duration": time.time() - start_time,
                "log_file": str(log_file_path) if 'log_file_path' in locals() else None,
                "error": "è¯„æµ‹è¶…æ—¶ (4å°æ—¶)"
            }
        except Exception as e:
            logger.error(f"ğŸ’¥ æ¨¡å‹ {model_name} å¤šä»»åŠ¡è¯„æµ‹å¼‚å¸¸: {e}")
            
            # å¼‚å¸¸æ—¶ä¹Ÿè¦è®°å½•æ—¥å¿—
            try:
                with open(log_file_path, 'a', encoding='utf-8') as log_file:
                    log_file.write(f"\n\n=== è¯„æµ‹å¼‚å¸¸ ===\n")
                    log_file.write(f"å¼‚å¸¸æ—¶é—´: {datetime.now().isoformat()}\n")
                    log_file.write(f"å¼‚å¸¸ä¿¡æ¯: {str(e)}\n")
                    log_file.write(f"å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}\n")
            except:
                pass
                
            return {
                "status": "error",
                "model_name": model_name,
                "lora_path": lora_path,
                "tasks": tasks,
                "task_count": len(tasks),
                "ray_gpu_ids": self.ray_gpu_ids,
                "duration": time.time() - start_time,
                "log_file": str(log_file_path) if 'log_file_path' in locals() else None,
                "error": str(e),
                "traceback": traceback.format_exc()
            }

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    @staticmethod
    def load_config(config_path: str) -> Dict[str, Any]:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {e}")
            raise
    
    @staticmethod
    def validate_config(config: Dict[str, Any]) -> bool:
        """éªŒè¯é…ç½®æ–‡ä»¶æ ¼å¼"""
        required_sections = ['models', 'evaluation']
        
        for section in required_sections:
            if section not in config:
                logger.error(f"âŒ é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€éƒ¨åˆ†: {section}")
                return False
        
        if not isinstance(config['models'], dict):
            logger.error("âŒ 'models' éƒ¨åˆ†åº”è¯¥æ˜¯å­—å…¸æ ¼å¼")
            return False
        
        if not isinstance(config['evaluation'], dict):
            logger.error("âŒ 'evaluation' éƒ¨åˆ†åº”è¯¥æ˜¯å­—å…¸æ ¼å¼")
            return False
        
        logger.info("âœ… é…ç½®æ–‡ä»¶æ ¼å¼éªŒè¯é€šè¿‡")
        return True
    
    @staticmethod
    def resolve_model_paths(config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£ææ¨¡å‹è·¯å¾„"""
        model_list = []
        models_config = config['models']
        
        base_model = models_config.get('base_model', 'meta-llama/Llama-2-7b-chat-hf')
        lora_models = models_config.get('lora_models', [])
        
        for model_entry in lora_models:
            if isinstance(model_entry, str):
                model_list.append({
                    'name': Path(model_entry).name,
                    'path': model_entry,
                    'base_model': base_model
                })
            elif isinstance(model_entry, dict):
                model_path = model_entry.get('path')
                model_name = model_entry.get('name', Path(model_path).name if model_path else 'unknown')
                model_base = model_entry.get('base_model', base_model)
                
                if model_path:
                    model_list.append({
                        'name': model_name,
                        'path': model_path,
                        'base_model': model_base
                    })
        
        # å¤„ç†ç›®å½•æ‰«æ
        if 'scan_directories' in models_config:
            scan_dirs = models_config['scan_directories']
            if not isinstance(scan_dirs, list):
                scan_dirs = [scan_dirs]
            
            for scan_dir in scan_dirs:
                scanned_models = ConfigManager._scan_directory_for_models(scan_dir, base_model)
                model_list.extend(scanned_models)
        
        logger.info(f"ğŸ” è§£æå¾—åˆ° {len(model_list)} ä¸ªæ¨¡å‹:")
        for model in model_list:
            logger.info(f"  ğŸ“ {model['name']} -> {model['path']}")
        
        return model_list
    
    @staticmethod
    def _scan_directory_for_models(directory: str, base_model: str) -> List[Dict[str, Any]]:
        """æ‰«æç›®å½•ä¸­çš„LoRAæ¨¡å‹"""
        models = []
        directory = Path(directory)
        
        if not directory.exists():
            logger.warning(f"âš ï¸ æ‰«æç›®å½•ä¸å­˜åœ¨: {directory}")
            return models
        
        for item in directory.iterdir():
            if item.is_dir():
                has_adapter_config = (item / 'adapter_config.json').exists()
                has_adapter_model = (item / 'adapter_model.bin').exists()
                has_safetensors = any(item.glob('adapter_model*.safetensors'))
                
                if has_adapter_config and (has_adapter_model or has_safetensors):
                    models.append({
                        'name': item.name,
                        'path': str(item),
                        'base_model': base_model
                    })
        
        return models

class OptimizedRayModelEvaluationManager:
    """ä¼˜åŒ–çš„Rayåˆ†å¸ƒå¼æ¨¡å‹è¯„æµ‹ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # ä»é…ç½®ä¸­è·å–Rayè®¾ç½®
        ray_config = config.get('ray', {})
        num_gpus = ray_config.get('num_gpus')
        
        # è‡ªåŠ¨æ£€æµ‹GPUæ•°é‡
        if num_gpus is None:
            try:
                import torch
                if torch.cuda.is_available():
                    num_gpus = torch.cuda.device_count()
                else:
                    num_gpus = 0
                    logger.warning("ç³»ç»Ÿä¸­æ²¡æœ‰å¯ç”¨çš„CUDA GPU")
            except ImportError:
                num_gpus = 0
                logger.warning("PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPU")
        
        self.num_gpus = max(1, num_gpus)
        self.evaluators = []
        
        # åˆå§‹åŒ–Ray
        if not ray.is_initialized():
            ray_init_config = ray_config.get('init_config', {})
            ray.init(num_gpus=self.num_gpus, **ray_init_config)
            logger.info(f"ğŸš€ Rayå·²åˆå§‹åŒ–ï¼Œå¯ç”¨GPUæ•°é‡: {self.num_gpus}")
        
        # è·å–å®é™…çš„Rayé›†ç¾¤èµ„æº
        cluster_resources = ray.cluster_resources()
        available_gpus = int(cluster_resources.get('GPU', 0))
        logger.info(f"ğŸ“Š Rayé›†ç¾¤èµ„æº: {cluster_resources}")
        logger.info(f"ğŸ® å¯ç”¨GPUæ•°é‡: {available_gpus}")
        
        # åˆ›å»ºè¯„æµ‹å™¨å®ä¾‹
        actual_evaluators = min(self.num_gpus, available_gpus)
        for i in range(actual_evaluators):
            evaluator = OptimizedModelEvaluator.remote()
            self.evaluators.append(evaluator)
        
        logger.info(f"âœ… å·²åˆ›å»º {len(self.evaluators)} ä¸ªOptimizedModelEvaluatorå®ä¾‹")
    
    def evaluate_all_models(self) -> List[Dict[str, Any]]:
        """è¯„æµ‹æ‰€æœ‰é…ç½®çš„æ¨¡å‹ - ä¼˜åŒ–ç‰ˆï¼šæ¯ä¸ªæ¨¡å‹ä¸€æ¬¡åŠ è½½è¯„æµ‹æ‰€æœ‰ä»»åŠ¡"""
        
        # è§£ææ¨¡å‹åˆ—è¡¨
        model_list = ConfigManager.resolve_model_paths(self.config)
        
        if not model_list:
            logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹")
            return []
        
        if not self.evaluators:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„è¯„æµ‹å™¨")
            return []
        
        # è·å–è¯„æµ‹é…ç½®
        eval_config = self.config['evaluation']
        tasks = eval_config.get('tasks', ['mmlu'])
        if isinstance(tasks, str):
            tasks = [tasks]
        
        output_dir = eval_config.get('output_dir', './ray_results')
        
        # åˆ›å»ºè¾“å‡ºç›®å½•å’Œæ—¥å¿—ç›®å½•
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        log_dir = Path(output_dir) / "logs"
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ“ ç»“æœç›®å½•: {output_dir}")
        logger.info(f"ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")
        
        # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºä¸€ä¸ªè¯„æµ‹ä»»åŠ¡ï¼ˆåŒ…å«æ‰€æœ‰ä»»åŠ¡ï¼‰
        all_tasks = []
        task_info = []
        
        for i, model_info in enumerate(model_list):
            # å‡†å¤‡è¯„æµ‹å‚æ•°
            eval_params = {
                'base_model': model_info['base_model'],
                'lora_path': model_info['path'],
                'tasks': tasks,  # ä¼ é€’æ‰€æœ‰ä»»åŠ¡
                'output_dir': output_dir,
                **eval_config.get('parameters', {})
            }
            
            # è½®è¯¢åˆ†é…è¯„æµ‹å™¨
            evaluator = self.evaluators[i % len(self.evaluators)]
            
            task_future = evaluator.evaluate_model_multi_tasks.remote(**eval_params)
            all_tasks.append(task_future)
            task_info.append({
                'model_name': model_info['name'],
                'tasks': tasks,
                'task_count': len(tasks),
                'path': model_info['path']
            })
        
        total_models = len(model_list)
        total_task_count = len(tasks) * total_models
        
        logger.info(f"ğŸ¯ ä¼˜åŒ–æ¨¡å¼ï¼šæ¯ä¸ªæ¨¡å‹ä¸€æ¬¡åŠ è½½è¯„æµ‹æ‰€æœ‰ä»»åŠ¡")
        logger.info(f"ğŸ“Š è¯„æµ‹ {total_models} ä¸ªæ¨¡å‹ï¼Œæ¯ä¸ªæ¨¡å‹ {len(tasks)} ä¸ªä»»åŠ¡")
        logger.info(f"ğŸ”§ ä½¿ç”¨ {len(self.evaluators)} ä¸ªè¯„æµ‹å™¨")
        logger.info(f"âš¡ æ€»ä»»åŠ¡æ•°: {total_task_count}ï¼Œä½†åªéœ€ {total_models} æ¬¡æ¨¡å‹åŠ è½½")
        logger.info(f"ğŸ“ æ¯ä¸ªè¯„æµ‹è¿›ç¨‹çš„æ—¥å¿—å°†ä¿å­˜åœ¨ {log_dir}")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        results = []
        completed = 0
        
        logger.info(f"â³ ç­‰å¾… {total_models} ä¸ªæ¨¡å‹è¯„æµ‹å®Œæˆ...")
        
        start_time = time.time()
        
        while all_tasks:
            # ç­‰å¾…è‡³å°‘ä¸€ä¸ªä»»åŠ¡å®Œæˆ
            ready_tasks, remaining_tasks = ray.wait(all_tasks, num_returns=1, timeout=60)
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for ready_future in ready_tasks:
                # æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡ä¿¡æ¯
                task_index = all_tasks.index(ready_future)
                info = task_info[task_index]
                
                try:
                    result = ray.get(ready_future)
                    results.append(result)
                    completed += 1
                    
                    status = result['status']
                    duration = result.get('duration', 0)
                    task_count = result.get('task_count', 0)
                    log_file = result.get('log_file', 'N/A')
                    
                    if status == 'success':
                        logger.info(f"âœ… [{completed}/{total_models}] {info['model_name']} "
                                  f"({task_count}ä¸ªä»»åŠ¡) è¯„æµ‹æˆåŠŸ ({duration:.1f}s)")
                        logger.info(f"   ğŸ“ æ—¥å¿—: {log_file}")
                    else:
                        error_msg = result.get('error', 'Unknown error')[:100]
                        logger.error(f"âŒ [{completed}/{total_models}] {info['model_name']} "
                                   f"({task_count}ä¸ªä»»åŠ¡) è¯„æµ‹å¤±è´¥: {error_msg}")
                        logger.error(f"   ğŸ“ æ—¥å¿—: {log_file}")
                        
                except Exception as e:
                    logger.error(f"ğŸ’¥ è·å–ä»»åŠ¡ç»“æœæ—¶å‡ºé”™: {e}")
                    completed += 1
                
                # ä»åˆ—è¡¨ä¸­ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
                all_tasks.remove(ready_future)
                task_info.pop(task_index)
            
            # æ˜¾ç¤ºè¿›åº¦
            if completed > 0 and completed % max(1, total_models // 10) == 0:
                elapsed = time.time() - start_time
                progress = completed / total_models * 100
                eta = (elapsed / completed) * (total_models - completed) if completed > 0 else 0
                logger.info(f"ğŸ“Š è¿›åº¦: {completed}/{total_models} ({progress:.1f}%) - "
                          f"å·²ç”¨æ—¶: {elapsed/60:.1f}min - é¢„è®¡å‰©ä½™: {eta/60:.1f}min")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        self.save_summary_results(results, output_dir)
        
        return results
    
    def save_summary_results(self, results: List[Dict[str, Any]], output_dir: str):
        """ä¿å­˜æ±‡æ€»ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary_file = Path(output_dir) / f"evaluation_summary_{timestamp}.json"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_models = len(results)
        successful = sum(1 for r in results if r['status'] == 'success')
        failed = sum(1 for r in results if r['status'] == 'failed')
        timeout = sum(1 for r in results if r['status'] == 'timeout')
        error = sum(1 for r in results if r['status'] == 'error')
        
        # è®¡ç®—æ€»è€—æ—¶å’Œä»»åŠ¡ç»Ÿè®¡
        total_duration = sum(r.get('duration', 0) for r in results)
        avg_duration = total_duration / total_models if total_models > 0 else 0
        total_task_evaluations = sum(r.get('task_count', 0) for r in results)
        
        # æå–æ‰€æœ‰ä»»åŠ¡åç§°
        all_tasks = set()
        for result in results:
            if result.get('tasks'):
                all_tasks.update(result['tasks'])
        all_tasks = list(all_tasks)
        
        # æ”¶é›†æ‰€æœ‰æ—¥å¿—æ–‡ä»¶è·¯å¾„
        log_files = []
        for result in results:
            if result.get('log_file'):
                log_files.append({
                    'model_name': result['model_name'],
                    'log_file': result['log_file'],
                    'status': result['status']
                })
        
        summary = {
            "evaluation_time": datetime.now().isoformat(),
            "optimization_mode": "multi_task_single_load",
            "config": self.config,
            "log_files": log_files,  # æ–°å¢ï¼šæ‰€æœ‰æ—¥å¿—æ–‡ä»¶ä¿¡æ¯
            "statistics": {
                "total_models": total_models,
                "successful_models": successful,
                "failed_models": failed,
                "timeout_models": timeout,
                "error_models": error,
                "success_rate": successful / total_models if total_models > 0 else 0,
                "total_duration_seconds": total_duration,
                "average_duration_per_model_seconds": avg_duration,
                "total_task_evaluations": total_task_evaluations,
                "tasks_evaluated": all_tasks,
                "efficiency_gain": f"~{len(all_tasks)}x faster than single-task mode"
            },
            "results": results
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
        logger.info(f"ğŸ“Š ä¼˜åŒ–æ¨¡å¼è¯„æµ‹ç»Ÿè®¡:")
        logger.info(f"   æ€»æ¨¡å‹æ•°: {total_models}")
        logger.info(f"   æˆåŠŸ: {successful} ({successful/total_models*100:.1f}%)")
        logger.info(f"   å¤±è´¥: {failed}, è¶…æ—¶: {timeout}, é”™è¯¯: {error}")
        logger.info(f"   æ€»è€—æ—¶: {total_duration:.1f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"   å¹³å‡æ¯æ¨¡å‹: {avg_duration:.1f}ç§’")
        logger.info(f"   æ€»ä»»åŠ¡è¯„æµ‹æ•°: {total_task_evaluations}")
        logger.info(f"   æ•ˆç‡æå‡: çº¦{len(all_tasks)}å€äºå•ä»»åŠ¡æ¨¡å¼")
        logger.info(f"ğŸ“ å…±ç”Ÿæˆ {len(log_files)} ä¸ªæ—¥å¿—æ–‡ä»¶")
        
        # ä¿å­˜ç®€åŒ–çš„ç»“æœè¡¨æ ¼
        self.save_results_table(results, output_dir)
    
    def save_results_table(self, results: List[Dict[str, Any]], output_dir: str):
        """ä¿å­˜ç»“æœè¡¨æ ¼"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        table_file = Path(output_dir) / f"results_table_{timestamp}.txt"
        
        with open(table_file, 'w', encoding='utf-8') as f:
            f.write(f"ä¼˜åŒ–ç‰ˆRayåˆ†å¸ƒå¼LoRAæ¨¡å‹è¯„æµ‹ç»“æœæ±‡æ€» (å¤šä»»åŠ¡å•æ¬¡åŠ è½½æ¨¡å¼)\n")
            f.write("=" * 150 + "\n")
            f.write(f"{'æ¨¡å‹åç§°':<40} {'ä»»åŠ¡æ•°':<8} {'çŠ¶æ€':<10} {'Ray GPU':<10} {'CUDAè®¾å¤‡':<12} {'è€—æ—¶(ç§’)':<10} {'æ—¥å¿—æ–‡ä»¶':<50} {'å¤‡æ³¨'}\n")
            f.write("-" * 150 + "\n")
            
            for result in results:
                model_name = result['model_name'][:38]
                task_count = str(result.get('task_count', 0))
                status = result['status']
                ray_gpus = str(result.get('ray_gpu_ids', 'N/A'))[:8]
                cuda_devices = str(result.get('cuda_visible_devices', 'N/A'))[:10]
                duration = f"{result.get('duration', 0):.1f}"
                log_file = result.get('log_file', 'N/A')
                # åªæ˜¾ç¤ºæ—¥å¿—æ–‡ä»¶åï¼Œä¸æ˜¾ç¤ºå®Œæ•´è·¯å¾„
                log_filename = Path(log_file).name if log_file != 'N/A' else 'N/A'
                log_filename = log_filename[:48]
                
                if status == 'success':
                    note = "âœ… å¤šä»»åŠ¡æˆåŠŸ"
                elif status == 'failed':
                    note = "âŒ " + str(result.get('error', ''))[:25]
                elif status == 'timeout':
                    note = "â° è¶…æ—¶"
                else:
                    note = "ğŸ’¥ å¼‚å¸¸"
                
                f.write(f"{model_name:<40} {task_count:<8} {status:<10} {ray_gpus:<10} {cuda_devices:<12} {duration:<10} {log_filename:<50} {note}\n")
            
            # æ·»åŠ æ—¥å¿—æ–‡ä»¶è¯´æ˜
            f.write("\n" + "=" * 150 + "\n")
            f.write("æ—¥å¿—æ–‡ä»¶è¯´æ˜:\n")
            f.write("æ‰€æœ‰è¯„æµ‹è¿‡ç¨‹çš„è¯¦ç»†æ—¥å¿—éƒ½ä¿å­˜åœ¨ logs/ ç›®å½•ä¸‹\n")
            f.write("æ¯ä¸ªæ¨¡å‹çš„è¯„æµ‹æ—¥å¿—åŒ…å«:\n")
            f.write("- è¯„æµ‹å¼€å§‹å’Œç»“æŸæ—¶é—´\n")
            f.write("- å®Œæ•´çš„å‘½ä»¤è¡Œå‚æ•°\n")
            f.write("- è¯„æµ‹è„šæœ¬çš„å®Œæ•´è¾“å‡º\n")
            f.write("- é”™è¯¯ä¿¡æ¯å’Œå †æ ˆè·Ÿè¸ª(å¦‚æœæœ‰)\n")
            f.write("- GPUå’Œç¯å¢ƒä¿¡æ¯\n")
        
        logger.info(f"ğŸ“‹ ç»“æœè¡¨æ ¼å·²ä¿å­˜åˆ°: {table_file}")
    
    def shutdown(self):
        """å…³é—­Ray"""
        if ray.is_initialized():
            ray.shutdown()
            logger.info("ğŸ”š Rayå·²å…³é—­")

def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç‰ˆRayåˆ†å¸ƒå¼LoRAæ¨¡å‹è¯„æµ‹ç³»ç»Ÿ - å¤šä»»åŠ¡å•æ¬¡åŠ è½½ + æ—¥å¿—ä¿å­˜")
    
    parser.add_argument("--config", "-c", type=str, required=True,
                        help="YAMLé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dry-run", action="store_true",
                        help="åªè§£æé…ç½®ï¼Œä¸æ‰§è¡Œè¯„æµ‹")
    
    return parser

def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()
    
    logger.info("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆRayåˆ†å¸ƒå¼LoRAæ¨¡å‹è¯„æµ‹ç³»ç»Ÿ (å¤šä»»åŠ¡å•æ¬¡åŠ è½½ + æ—¥å¿—ä¿å­˜)")
    logger.info(f"ğŸ“„ é…ç½®æ–‡ä»¶: {args.config}")
    
    try:
        # åŠ è½½å’ŒéªŒè¯é…ç½®
        config = ConfigManager.load_config(args.config)
        
        if not ConfigManager.validate_config(config):
            logger.error("âŒ é…ç½®æ–‡ä»¶éªŒè¯å¤±è´¥")
            return
        
        # å¦‚æœæ˜¯dry-runæ¨¡å¼ï¼Œåªæ˜¾ç¤ºè§£æç»“æœ
        if args.dry_run:
            logger.info("ğŸ” Dry-runæ¨¡å¼ - è§£æé…ç½®:")
            model_list = ConfigManager.resolve_model_paths(config)
            eval_config = config['evaluation']
            tasks = eval_config.get('tasks', ['mmlu'])
            output_dir = eval_config.get('output_dir', './ray_results')
            log_dir = Path(output_dir) / "logs"
            
            logger.info(f"ğŸ“Š å°†è¯„æµ‹ {len(model_list)} ä¸ªæ¨¡å‹:")
            for model in model_list:
                logger.info(f"  ğŸ“ {model['name']} ({model['base_model']})")
            
            logger.info(f"ğŸ¯ è¯„æµ‹ä»»åŠ¡: {tasks}")
            logger.info(f"ğŸ’¾ è¾“å‡ºç›®å½•: {output_dir}")
            logger.info(f"ğŸ“ æ—¥å¿—ç›®å½•: {log_dir}")
            logger.info(f"âš™ï¸ è¯„æµ‹å‚æ•°: {eval_config.get('parameters', {})}")
            
            logger.info(f"âš¡ ä¼˜åŒ–æ¨¡å¼: æ¯ä¸ªæ¨¡å‹ä¸€æ¬¡åŠ è½½ï¼Œè¯„æµ‹ {len(tasks)} ä¸ªä»»åŠ¡")
            logger.info(f"ğŸ“ˆ æ€»æ¨¡å‹æ•°: {len(model_list)}")
            logger.info(f"ğŸ”¥ æ•ˆç‡æå‡: çº¦{len(tasks)}å€äºå•ä»»åŠ¡æ¨¡å¼")
            logger.info(f"ğŸ“ æ¯ä¸ªæ¨¡å‹å°†ç”Ÿæˆç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶")
            return
        
        # åˆ›å»ºç®¡ç†å™¨å¹¶å¼€å§‹è¯„æµ‹
        manager = OptimizedRayModelEvaluationManager(config)
        
        start_time = time.time()
        results = manager.evaluate_all_models()
        total_time = time.time() - start_time
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if r['status'] == 'success')
        total_models = len(results)
        total_task_evaluations = sum(r.get('task_count', 0) for r in results)
        
        # ç»Ÿè®¡æ—¥å¿—æ–‡ä»¶
        log_files_created = sum(1 for r in results if r.get('log_file'))
        
        logger.info(f"ğŸ‰ ä¼˜åŒ–ç‰ˆè¯„æµ‹å®Œæˆï¼")
        logger.info(f"ğŸ“Š æ€»è€—æ—¶: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
        logger.info(f"âœ… æˆåŠŸç‡: {successful}/{total_models} ({successful/total_models*100:.1f}%)")
        logger.info(f"ğŸ”¥ æ€»ä»»åŠ¡è¯„æµ‹æ•°: {total_task_evaluations}")
        logger.info(f"ğŸ“ ç”Ÿæˆæ—¥å¿—æ–‡ä»¶æ•°: {log_files_created}")
        logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {config['evaluation'].get('output_dir', './ray_results')}")
        logger.info(f"ğŸ“ è¯¦ç»†æ—¥å¿—ä¿å­˜åœ¨: {config['evaluation'].get('output_dir', './ray_results')}/logs/")
        
    except Exception as e:
        logger.error(f"âŒ è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        if 'manager' in locals():
            manager.shutdown()

if __name__ == "__main__":
    main()