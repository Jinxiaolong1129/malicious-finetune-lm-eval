#!/usr/bin/env python3
# 


import os
import ray
import time
import json
import torch
import argparse
from pathlib import Path
from typing import List, Dict, Any
import subprocess
import logging
from datetime import datetime
import traceback

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ray_evaluation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@ray.remote(num_gpus=1)
class ModelEvaluator:
    """å•ä¸ªæ¨¡å‹è¯„æµ‹å™¨ - æ¯ä¸ªå®ä¾‹å ç”¨1å¼ GPU"""
    
    def __init__(self):
        # âœ… æ­£ç¡®åšæ³•ï¼šåªè·å–ä¿¡æ¯ï¼Œä¸ä¿®æ”¹ç¯å¢ƒå˜é‡
        self.ray_gpu_ids = ray.get_gpu_ids()
        self.cuda_visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', 'unknown')
        
        logger.info(f"âœ… ModelEvaluatoråˆå§‹åŒ–:")
        logger.info(f"   Rayåˆ†é…çš„GPU IDs: {self.ray_gpu_ids}")
        logger.info(f"   CUDA_VISIBLE_DEVICES: {self.cuda_visible_devices}")
        
        # éªŒè¯CUDAè®¾ç½®
        if torch.cuda.is_available():
            device_count = torch.cuda.device_count()
            current_device = torch.cuda.current_device()
            logger.info(f"   PyTorchå¯è§GPUæ•°é‡: {device_count}")
            logger.info(f"   PyTorchå½“å‰è®¾å¤‡: {current_device}")
            
            # è·å–GPUä¿¡æ¯
            for i in range(device_count):
                gpu_name = torch.cuda.get_device_name(i)
                logger.info(f"   GPU {i}: {gpu_name}")
        else:
            logger.warning("âš ï¸ CUDAä¸å¯ç”¨")
    
    def evaluate_model(self, 
                      base_model: str,
                      lora_path: str, 
                      task: str = "mmlu",
                      output_dir: str = "./results",
                      tensor_parallel_size: int = 1,
                      gpu_memory_utilization: float = 0.8,
                      num_fewshot: int = 0,
                      **eval_kwargs) -> Dict[str, Any]:
        """è¯„æµ‹å•ä¸ªæ¨¡å‹"""
        
        start_time = time.time()
        model_name = Path(lora_path).name
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹è¯„æµ‹æ¨¡å‹: {model_name}")
            logger.info(f"   å½“å‰CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES')}")
            
            # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_file = Path(output_dir) / f"{model_name}_{task}_results.json"
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            cmd = [
                "/home/jin509/anaconda3/envs/malicious_finetune/bin/python", 
                "ray-run_evaluation.py",
                "--base-model", base_model,
                "--lora-path", lora_path,
                "--task", task,
                "--output", str(output_file),
                "--tensor-parallel-size", str(tensor_parallel_size),
                "--gpu-memory-utilization", str(gpu_memory_utilization),
                "--num-fewshot", str(num_fewshot)
            ]
            
            # æ·»åŠ å…¶ä»–å‚æ•°
            for key, value in eval_kwargs.items():
                if value is not None:
                    cmd.extend([f"--{key.replace('_', '-')}", str(value)])
            
            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd[:8])}...")  # åªæ˜¾ç¤ºå‰å‡ ä¸ªå‚æ•°
            
            # âœ… æ­£ç¡®åšæ³•ï¼šç›´æ¥ç»§æ‰¿ç¯å¢ƒå˜é‡ï¼Œä¸æ‰‹åŠ¨è®¾ç½®CUDA_VISIBLE_DEVICES
            env = os.environ.copy()
            # ä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®CUDA_VISIBLE_DEVICESï¼ŒRayå·²ç»è®¾ç½®å¥½äº†
            
            # æ‰§è¡Œè¯„æµ‹
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=7200,  # 2å°æ—¶è¶…æ—¶
                env=env  # ç›´æ¥ä½¿ç”¨ç»§æ‰¿çš„ç¯å¢ƒå˜é‡
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            if result.returncode == 0:
                logger.info(f"âœ… æ¨¡å‹ {model_name} è¯„æµ‹æˆåŠŸ (è€—æ—¶: {duration:.1f}ç§’)")
                
                # å°è¯•è¯»å–ç»“æœæ–‡ä»¶ - æŸ¥æ‰¾å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶
                results_data = None
                result_files = list(output_file.parent.glob(f"{output_file.stem}_*.json"))
                
                if result_files:
                    # ä½¿ç”¨æœ€æ–°çš„ç»“æœæ–‡ä»¶
                    latest_file = max(result_files, key=lambda x: x.stat().st_mtime)
                    try:
                        with open(latest_file, 'r', encoding='utf-8') as f:
                            results_data = json.load(f)
                        logger.info(f"âœ… è¯»å–ç»“æœæ–‡ä»¶: {latest_file}")
                    except Exception as e:
                        logger.warning(f"æ— æ³•è¯»å–ç»“æœæ–‡ä»¶ {latest_file}: {e}")
                elif output_file.exists():
                    try:
                        with open(output_file, 'r', encoding='utf-8') as f:
                            results_data = json.load(f)
                    except Exception as e:
                        logger.warning(f"æ— æ³•è¯»å–ç»“æœæ–‡ä»¶ {output_file}: {e}")
                
                return {
                    "status": "success",
                    "model_name": model_name,
                    "lora_path": lora_path,
                    "task": task,
                    "ray_gpu_ids": self.ray_gpu_ids,
                    "cuda_visible_devices": self.cuda_visible_devices,
                    "duration": duration,
                    "output_file": str(output_file),
                    "results": results_data,
                    "stdout": result.stdout[-1000:],  # åªä¿ç•™æœ€å1000å­—ç¬¦
                    "stderr": result.stderr[-1000:] if result.stderr else ""
                }
            else:
                logger.error(f"âŒ æ¨¡å‹ {model_name} è¯„æµ‹å¤±è´¥")
                logger.error(f"é”™è¯¯è¾“å‡º: {result.stderr}")
                
                return {
                    "status": "failed",
                    "model_name": model_name,
                    "lora_path": lora_path,
                    "task": task,
                    "ray_gpu_ids": self.ray_gpu_ids,
                    "cuda_visible_devices": self.cuda_visible_devices,
                    "duration": duration,
                    "error": result.stderr,
                    "stdout": result.stdout,
                    "returncode": result.returncode
                }
                
        except subprocess.TimeoutExpired:
            logger.error(f"â° æ¨¡å‹ {model_name} è¯„æµ‹è¶…æ—¶")
            return {
                "status": "timeout",
                "model_name": model_name,
                "lora_path": lora_path,
                "task": task,
                "ray_gpu_ids": self.ray_gpu_ids,
                "duration": time.time() - start_time,
                "error": "è¯„æµ‹è¶…æ—¶"
            }
        except Exception as e:
            logger.error(f"ğŸ’¥ æ¨¡å‹ {model_name} è¯„æµ‹å¼‚å¸¸: {e}")
            return {
                "status": "error",
                "model_name": model_name,
                "lora_path": lora_path,
                "task": task,
                "ray_gpu_ids": self.ray_gpu_ids,
                "duration": time.time() - start_time,
                "error": str(e),
                "traceback": traceback.format_exc()
            }

class RayModelEvaluationManager:
    """Rayåˆ†å¸ƒå¼æ¨¡å‹è¯„æµ‹ç®¡ç†å™¨"""
    
    def __init__(self, num_gpus: int = None):
        # âœ… æ”¹è¿›ï¼šè‡ªåŠ¨æ£€æµ‹GPUæ•°é‡
        if num_gpus is None:
            # å°è¯•æ£€æµ‹ç³»ç»ŸGPUæ•°é‡
            try:
                import torch
                if torch.cuda.is_available():
                    num_gpus = torch.cuda.device_count()
                else:
                    num_gpus = 0
                    logger.warning("ç³»ç»Ÿä¸­æ²¡æœ‰å¯ç”¨çš„CUDA GPU")
            except ImportError:
                num_gpus = 0
                logger.warning("PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPU")
        
        self.num_gpus = max(1, num_gpus)  # è‡³å°‘ä¿ç•™1ä¸ªç”¨äºCPU
        self.evaluators = []
        
        # åˆå§‹åŒ–Ray
        if not ray.is_initialized():
            ray.init(num_gpus=self.num_gpus)
            logger.info(f"ğŸš€ Rayå·²åˆå§‹åŒ–ï¼Œå¯ç”¨GPUæ•°é‡: {self.num_gpus}")
        
        # è·å–å®é™…çš„Rayé›†ç¾¤èµ„æº
        cluster_resources = ray.cluster_resources()
        available_gpus = int(cluster_resources.get('GPU', 0))
        logger.info(f"ğŸ“Š Rayé›†ç¾¤èµ„æº: {cluster_resources}")
        logger.info(f"ğŸ® å¯ç”¨GPUæ•°é‡: {available_gpus}")
        
        # åˆ›å»ºè¯„æµ‹å™¨å®ä¾‹ï¼ˆæ•°é‡ä¸è¶…è¿‡å¯ç”¨GPUæ•°ï¼‰
        actual_evaluators = min(self.num_gpus, available_gpus)
        for i in range(actual_evaluators):
            evaluator = ModelEvaluator.remote()
            self.evaluators.append(evaluator)
        
        logger.info(f"âœ… å·²åˆ›å»º {len(self.evaluators)} ä¸ªModelEvaluatorå®ä¾‹")
    
    def find_models(self, models_dir: str) -> List[str]:
        """æ‰«ææ¨¡å‹æ–‡ä»¶å¤¹ï¼Œæ‰¾åˆ°æ‰€æœ‰LoRAæ¨¡å‹"""
        models_dir = Path(models_dir)
        
        if not models_dir.exists():
            raise ValueError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {models_dir}")
        
        # æŸ¥æ‰¾åŒ…å«LoRAæƒé‡çš„ç›®å½•
        model_paths = []
        for item in models_dir.iterdir():
            if item.is_dir():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«LoRAå¿…éœ€æ–‡ä»¶
                has_adapter_config = (item / 'adapter_config.json').exists()
                has_adapter_model = (item / 'adapter_model.bin').exists()
                has_safetensors = any(item.glob('adapter_model*.safetensors'))
                
                if has_adapter_config and (has_adapter_model or has_safetensors):
                    model_paths.append(str(item))
        
        logger.info(f"ğŸ” åœ¨ {models_dir} ä¸­æ‰¾åˆ° {len(model_paths)} ä¸ªLoRAæ¨¡å‹:")
        for path in model_paths:
            logger.info(f"  ğŸ“ {Path(path).name}")
        
        return sorted(model_paths)
    
    def evaluate_all_models(self,
                           models_dir: str,
                           base_model: str,
                           task: str = "mmlu",
                           output_dir: str = "./ray_results",
                           tensor_parallel_size: int = 1,
                           gpu_memory_utilization: float = 0.8,
                           num_fewshot: int = 0,
                           **eval_kwargs) -> List[Dict[str, Any]]:
        """è¯„æµ‹æ‰€æœ‰æ¨¡å‹"""
        
        # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹
        model_paths = self.find_models(models_dir)
        
        if not model_paths:
            logger.warning("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•LoRAæ¨¡å‹")
            return []
        
        if not self.evaluators:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„è¯„æµ‹å™¨")
            return []
        
        logger.info(f"ğŸ¯ å¼€å§‹è¯„æµ‹ {len(model_paths)} ä¸ªæ¨¡å‹ï¼Œä½¿ç”¨ {len(self.evaluators)} ä¸ªè¯„æµ‹å™¨")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        pending_tasks = []
        for i, model_path in enumerate(model_paths):
            # è½®è¯¢åˆ†é…è¯„æµ‹å™¨
            evaluator = self.evaluators[i % len(self.evaluators)]
            
            task_future = evaluator.evaluate_model.remote(
                base_model=base_model,
                lora_path=model_path,
                task=task,
                output_dir=output_dir,
                tensor_parallel_size=tensor_parallel_size,
                gpu_memory_utilization=gpu_memory_utilization,
                num_fewshot=num_fewshot,
                **eval_kwargs
            )
            
            pending_tasks.append((task_future, Path(model_path).name))
            logger.info(f"ğŸ“¤ å·²æäº¤ä»»åŠ¡ {i+1}/{len(model_paths)}: {Path(model_path).name}")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        results = []
        completed = 0
        total = len(pending_tasks)
        
        logger.info(f"â³ ç­‰å¾… {total} ä¸ªè¯„æµ‹ä»»åŠ¡å®Œæˆ...")
        
        while pending_tasks:
            # ç­‰å¾…è‡³å°‘ä¸€ä¸ªä»»åŠ¡å®Œæˆ
            ready_futures = [task[0] for task in pending_tasks]
            ready_tasks, remaining_tasks = ray.wait(ready_futures, num_returns=1, timeout=60)
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for ready_future in ready_tasks:
                # æ‰¾åˆ°å¯¹åº”çš„ä»»åŠ¡
                for i, (task_future, model_name) in enumerate(pending_tasks):
                    if task_future == ready_future:
                        try:
                            result = ray.get(ready_future)
                            results.append(result)
                            completed += 1
                            
                            status = result['status']
                            duration = result.get('duration', 0)
                            
                            if status == 'success':
                                logger.info(f"âœ… [{completed}/{total}] {model_name} è¯„æµ‹æˆåŠŸ ({duration:.1f}s)")
                            else:
                                error_msg = result.get('error', 'Unknown error')[:100]
                                logger.error(f"âŒ [{completed}/{total}] {model_name} è¯„æµ‹å¤±è´¥: {error_msg}")
                                
                        except Exception as e:
                            logger.error(f"ğŸ’¥ è·å–ä»»åŠ¡ç»“æœæ—¶å‡ºé”™: {e}")
                            completed += 1
                        
                        # ä»å¾…å¤„ç†åˆ—è¡¨ä¸­ç§»é™¤
                        pending_tasks.pop(i)
                        break
            
            # æ˜¾ç¤ºè¿›åº¦
            if completed > 0 and completed % max(1, total // 10) == 0:
                progress = completed / total * 100
                logger.info(f"ğŸ“Š è¿›åº¦: {completed}/{total} ({progress:.1f}%)")
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        self.save_summary_results(results, output_dir, task)
        
        return results
    
    def save_summary_results(self, results: List[Dict[str, Any]], output_dir: str, task: str):
        """ä¿å­˜æ±‡æ€»ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary_file = Path(output_dir) / f"evaluation_summary_{task}_{timestamp}.json"
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_models = len(results)
        successful = sum(1 for r in results if r['status'] == 'success')
        failed = sum(1 for r in results if r['status'] == 'failed')
        timeout = sum(1 for r in results if r['status'] == 'timeout')
        error = sum(1 for r in results if r['status'] == 'error')
        
        # è®¡ç®—æ€»è€—æ—¶
        total_duration = sum(r.get('duration', 0) for r in results)
        avg_duration = total_duration / total_models if total_models > 0 else 0
        
        summary = {
            "evaluation_time": datetime.now().isoformat(),
            "task": task,
            "statistics": {
                "total_models": total_models,
                "successful": successful,
                "failed": failed,
                "timeout": timeout,
                "error": error,
                "success_rate": successful / total_models if total_models > 0 else 0,
                "total_duration_seconds": total_duration,
                "average_duration_seconds": avg_duration
            },
            "results": results
        }
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
        logger.info(f"ğŸ“Š è¯„æµ‹ç»Ÿè®¡:")
        logger.info(f"   æ€»æ¨¡å‹æ•°: {total_models}")
        logger.info(f"   æˆåŠŸ: {successful} ({successful/total_models*100:.1f}%)")
        logger.info(f"   å¤±è´¥: {failed}")
        logger.info(f"   è¶…æ—¶: {timeout}")
        logger.info(f"   é”™è¯¯: {error}")
        logger.info(f"   æ€»è€—æ—¶: {total_duration:.1f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
        logger.info(f"   å¹³å‡è€—æ—¶: {avg_duration:.1f}ç§’")
        
        # ä¿å­˜ç®€åŒ–çš„ç»“æœè¡¨æ ¼
        self.save_results_table(results, output_dir, task)
    
    def save_results_table(self, results: List[Dict[str, Any]], output_dir: str, task: str):
        """ä¿å­˜ç»“æœè¡¨æ ¼"""
        table_file = Path(output_dir) / f"results_table_{task}.txt"
        
        with open(table_file, 'w', encoding='utf-8') as f:
            f.write(f"LoRAæ¨¡å‹è¯„æµ‹ç»“æœæ±‡æ€» - ä»»åŠ¡: {task}\n")
            f.write("=" * 100 + "\n")
            f.write(f"{'æ¨¡å‹åç§°':<40} {'çŠ¶æ€':<10} {'Ray GPU':<10} {'CUDAè®¾å¤‡':<12} {'è€—æ—¶(ç§’)':<10} {'å¤‡æ³¨'}\n")
            f.write("-" * 100 + "\n")
            
            for result in results:
                model_name = result['model_name'][:38]
                status = result['status']
                ray_gpus = str(result.get('ray_gpu_ids', 'N/A'))[:8]
                cuda_devices = str(result.get('cuda_visible_devices', 'N/A'))[:10]
                duration = f"{result.get('duration', 0):.1f}"
                
                if status == 'success':
                    note = "âœ…"
                elif status == 'failed':
                    note = "âŒ " + str(result.get('error', ''))[:20]
                elif status == 'timeout':
                    note = "â° è¶…æ—¶"
                else:
                    note = "ğŸ’¥ å¼‚å¸¸"
                
                f.write(f"{model_name:<40} {status:<10} {ray_gpus:<10} {cuda_devices:<12} {duration:<10} {note}\n")
        
        logger.info(f"ğŸ“‹ ç»“æœè¡¨æ ¼å·²ä¿å­˜åˆ°: {table_file}")
    
    def shutdown(self):
        """å…³é—­Ray"""
        if ray.is_initialized():
            ray.shutdown()
            logger.info("ğŸ”š Rayå·²å…³é—­")

def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description="Rayåˆ†å¸ƒå¼LoRAæ¨¡å‹è¯„æµ‹ç³»ç»Ÿ")
    
    parser.add_argument("--models-dir", type=str, required=True,
                        help="åŒ…å«æ‰€æœ‰LoRAæ¨¡å‹çš„ç›®å½•")
    parser.add_argument("--base-model", type=str, 
                        default="meta-llama/Llama-2-7b-chat-hf",
                        help="åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--task", type=str, default="mmlu",
                        choices=["mmlu", "humaneval", "truthfulqa", "all"],
                        help="è¯„æµ‹ä»»åŠ¡")
    parser.add_argument("--output-dir", type=str, default="./ray_results",
                        help="ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--num-gpus", type=int, default=None,
                        help="ä½¿ç”¨çš„GPUæ•°é‡ (é»˜è®¤è‡ªåŠ¨æ£€æµ‹)")
    parser.add_argument("--tensor-parallel-size", type=int, default=1,
                        help="vLLM tensor parallel size")
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.8,
                        help="GPUå†…å­˜ä½¿ç”¨ç‡")
    parser.add_argument("--num-fewshot", type=int, default=0,
                        help="Few-shotæ•°é‡")
    parser.add_argument("--batch-size", type=str, default="auto",
                        help="æ‰¹å¤„ç†å¤§å°")
    
    return parser

def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()
    
    logger.info("ğŸš€ å¯åŠ¨Rayåˆ†å¸ƒå¼LoRAæ¨¡å‹è¯„æµ‹ç³»ç»Ÿ")
    logger.info(f"ğŸ“ æ¨¡å‹ç›®å½•: {args.models_dir}")
    logger.info(f"ğŸ¤– åŸºç¡€æ¨¡å‹: {args.base_model}")
    logger.info(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {args.task}")
    logger.info(f"ğŸ’¾ è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.info(f"ğŸ® æŒ‡å®šGPUæ•°é‡: {args.num_gpus or 'è‡ªåŠ¨æ£€æµ‹'}")
    logger.info(f"âš¡ Tensor Parallel: {args.tensor_parallel_size}")
    logger.info(f"ğŸ§  GPUå†…å­˜ä½¿ç”¨ç‡: {args.gpu_memory_utilization}")
    logger.info(f"ğŸ¯ Few-shotæ•°é‡: {args.num_fewshot}")
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = RayModelEvaluationManager(num_gpus=args.num_gpus)
    
    try:
        # æ„å»ºè¯„æµ‹å‚æ•°
        eval_kwargs = {}
        if args.batch_size != "auto":
            eval_kwargs["batch_size"] = args.batch_size
        
        # å¼€å§‹è¯„æµ‹
        start_time = time.time()
        results = manager.evaluate_all_models(
            models_dir=args.models_dir,
            base_model=args.base_model,
            task=args.task,
            output_dir=args.output_dir,
            tensor_parallel_size=args.tensor_parallel_size,
            gpu_memory_utilization=args.gpu_memory_utilization,
            num_fewshot=args.num_fewshot,
            **eval_kwargs
        )
        
        total_time = time.time() - start_time
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if r['status'] == 'success')
        total = len(results)
        
        logger.info(f"ğŸ‰ è¯„æµ‹å®Œæˆï¼")
        logger.info(f"ğŸ“Š æ€»è€—æ—¶: {total_time:.1f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)")
        logger.info(f"âœ… æˆåŠŸç‡: {successful}/{total} ({successful/total*100:.1f}%)")
        logger.info(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.output_dir}")
        
    except Exception as e:
        logger.error(f"âŒ è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æº
        manager.shutdown()

if __name__ == "__main__":
    main()