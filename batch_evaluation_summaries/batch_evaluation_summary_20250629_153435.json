{
  "evaluation_time": "2025-06-29T15:34:35.878594",
  "config_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/lm_eval_model_configs_pre_alignment.json",
  "tasks": "all",
  "num_gpus_per_task": 1,
  "tensor_parallel_size": 1,
  "total_experiments": 6,
  "pending_experiments": 6,
  "total_duration_seconds": 2296.1191992759705,
  "total_duration_minutes": 38.26865332126617,
  "output_mode": "lora_directories",
  "scheduling_mode": "ray_auto_scheduling",
  "progress_file": "/data3/user/jin509/malicious-finetuning/experiments-sft_stage/lm_eval_model_configs_pre_alignment_progress.csv",
  "results_summary": {
    "successful": 6,
    "failed": 0,
    "success_rate": 100.0
  },
  "detailed_results": [
    {
      "experiment_name": "booster_pre_alignment_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000",
      "status": "completed",
      "start_time": "2025-06-29T14:56:22.295552",
      "end_time": "2025-06-29T15:06:40.508509",
      "duration": 618.2129547595978,
      "duration_minutes": 10.303549245993297,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000/log_lm_eval/booster_pre_alignment_final_all_gpu1.log",
      "stdout": "   super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/merged_lora_i9p9t81p\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4134857",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "repnoise_pre_alignment_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign100-harmful100-epoch5-rho0.001-lamb0.1",
      "status": "completed",
      "start_time": "2025-06-29T14:56:22.355785",
      "end_time": "2025-06-29T15:08:25.327391",
      "duration": 722.9716041088104,
      "duration_minutes": 12.04952673514684,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign100-harmful100-epoch5-rho0.001-lamb0.1/log_lm_eval/repnoise_pre_alignment_final_all_gpu1.log",
      "stdout": "  super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/merged_lora_aeim4564\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4134852",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "booster_pre_alignment_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_alpaca-train_alpaca-eval_alpaca/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000",
      "status": "completed",
      "start_time": "2025-06-29T14:56:22.356098",
      "end_time": "2025-06-29T15:10:12.100456",
      "duration": 829.7443342208862,
      "duration_minutes": 13.82907223701477,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_alpaca-train_alpaca-eval_alpaca/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000/log_lm_eval/booster_pre_alignment_final_all_gpu1.log",
      "stdout": ", in __init__\n    super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_alpaca-train_alpaca-eval_alpaca/merged_lora_qsq4a7qb\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4134850",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "repnoise_pre_alignment_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_alpaca-train_alpaca-eval_alpaca/benign100-harmful100-epoch5-rho0.001-lamb0.1",
      "status": "completed",
      "start_time": "2025-06-29T15:06:42.838518",
      "end_time": "2025-06-29T15:17:40.722005",
      "duration": 657.8834640979767,
      "duration_minutes": 10.964724401632944,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_alpaca-train_alpaca-eval_alpaca/benign100-harmful100-epoch5-rho0.001-lamb0.1/log_lm_eval/repnoise_pre_alignment_final_all_gpu1.log",
      "stdout": "n __init__\n    super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_alpaca-train_alpaca-eval_alpaca/merged_lora_dhy9b58f\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 172.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4134864",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "vaccine_pre_alignment_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/data50-epoch5-rho2",
      "status": "completed",
      "start_time": "2025-06-29T15:08:28.186372",
      "end_time": "2025-06-29T15:20:39.494701",
      "duration": 731.3082571029663,
      "duration_minutes": 12.188470951716106,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/data50-epoch5-rho2/log_lm_eval/vaccine_pre_alignment_final_all_gpu1.log",
      "stdout": "   super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/merged_lora_kvbzizod\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4134866",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "vaccine_pre_alignment_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_alpaca-train_alpaca-eval_alpaca/data50-epoch5-rho2",
      "status": "completed",
      "start_time": "2025-06-29T15:20:41.792803",
      "end_time": "2025-06-29T15:34:35.853478",
      "duration": 834.0606663227081,
      "duration_minutes": 13.901011105378469,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_alpaca-train_alpaca-eval_alpaca/data50-epoch5-rho2/log_lm_eval/vaccine_pre_alignment_final_all_gpu1.log",
      "stdout": ", in __init__\n    super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/pre_alignment_model/Llama-2-7b-chat-hf-lora-r64-pre_align_alpaca-train_alpaca-eval_alpaca/merged_lora_j9c52fcm\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4134851",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    }
  ]
}