{
  "evaluation_time": "2025-06-29T14:56:09.701077",
  "config_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/lm_eval_model_configs_final.json",
  "tasks": "all",
  "num_gpus_per_task": 1,
  "tensor_parallel_size": 1,
  "total_experiments": 12,
  "pending_experiments": 12,
  "total_duration_seconds": 3731.3214027881622,
  "total_duration_minutes": 62.18869004646937,
  "output_mode": "lora_directories",
  "scheduling_mode": "ray_auto_scheduling",
  "progress_file": "/data3/user/jin509/malicious-finetuning/experiments-sft_stage/lm_eval_model_configs_final_progress.csv",
  "results_summary": {
    "successful": 12,
    "failed": 0,
    "success_rate": 100.0
  },
  "detailed_results": [
    {
      "experiment_name": "booster_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000",
      "status": "completed",
      "start_time": "2025-06-29T13:54:01.884221",
      "end_time": "2025-06-29T14:07:50.173030",
      "duration": 828.2888023853302,
      "duration_minutes": 13.804813373088837,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000/log_lm_eval/booster_final_all_gpu1.log",
      "stdout": "__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/merged_lora_zfp19auv\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009329",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "booster_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000",
      "status": "completed",
      "start_time": "2025-06-29T13:54:01.678079",
      "end_time": "2025-06-29T14:07:57.407158",
      "duration": 835.7290744781494,
      "duration_minutes": 13.928817907969156,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000/log_lm_eval/booster_final_all_gpu1.log",
      "stdout": " super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/merged_lora_h45d339r\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009326",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "booster_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000",
      "status": "completed",
      "start_time": "2025-06-29T13:54:01.843693",
      "end_time": "2025-06-29T14:08:05.559181",
      "duration": 843.7154846191406,
      "duration_minutes": 14.061924743652344,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000/log_lm_eval/booster_final_all_gpu1.log",
      "stdout": "__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/merged_lora_4yf5r08h\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009330",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "booster_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000",
      "status": "completed",
      "start_time": "2025-06-29T14:07:52.868073",
      "end_time": "2025-06-29T14:19:21.986667",
      "duration": 689.1185717582703,
      "duration_minutes": 11.485309529304505,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/benign50-harmful50-epoch5-lamb5-alpha0.1-rho0.1-perturbFalse-metaTrue-guide10000/log_lm_eval/booster_final_all_gpu1.log",
      "stdout": " super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/booster/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/merged_lora_7zj5l0mk\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009334",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "repnoise_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign100-harmful100-epoch5-rho0.001-lamb0.1",
      "status": "completed",
      "start_time": "2025-06-29T14:08:00.659407",
      "end_time": "2025-06-29T14:19:40.999718",
      "duration": 700.3402876853943,
      "duration_minutes": 11.672338128089905,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign100-harmful100-epoch5-rho0.001-lamb0.1/log_lm_eval/repnoise_final_all_gpu1.log",
      "stdout": "_(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/merged_lora_alhr_1ld\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009324",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "repnoise_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/benign100-harmful100-epoch5-rho0.001-lamb0.1",
      "status": "completed",
      "start_time": "2025-06-29T14:19:43.885623",
      "end_time": "2025-06-29T14:32:20.754074",
      "duration": 756.8684475421906,
      "duration_minutes": 12.614474125703175,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/benign100-harmful100-epoch5-rho0.001-lamb0.1/log_lm_eval/repnoise_final_all_gpu1.log",
      "stdout": "super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/merged_lora_8zg8pecz\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009337",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "repnoise_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign100-harmful100-epoch5-rho0.001-lamb0.1",
      "status": "completed",
      "start_time": "2025-06-29T14:19:43.772168",
      "end_time": "2025-06-29T14:32:31.411618",
      "duration": 767.6394488811493,
      "duration_minutes": 12.793990814685822,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/benign100-harmful100-epoch5-rho0.001-lamb0.1/log_lm_eval/repnoise_final_all_gpu1.log",
      "stdout": "_(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/merged_lora_1opgjbu1\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009346",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "repnoise_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/benign100-harmful100-epoch5-rho0.001-lamb0.1",
      "status": "completed",
      "start_time": "2025-06-29T14:19:43.758418",
      "end_time": "2025-06-29T14:32:39.000450",
      "duration": 775.242032289505,
      "duration_minutes": 12.920700538158417,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/benign100-harmful100-epoch5-rho0.001-lamb0.1/log_lm_eval/repnoise_final_all_gpu1.log",
      "stdout": "super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/repnoise/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/merged_lora_yrcunrv9\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009328",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "vaccine_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/data50-epoch5-rho2",
      "status": "completed",
      "start_time": "2025-06-29T14:32:23.111286",
      "end_time": "2025-06-29T14:43:56.579092",
      "duration": 693.4677836894989,
      "duration_minutes": 11.557796394824981,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/data50-epoch5-rho2/log_lm_eval/vaccine_final_all_gpu1.log",
      "stdout": "__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/merged_lora_5x81ovc1\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009331",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "vaccine_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/data50-epoch5-rho2",
      "status": "completed",
      "start_time": "2025-06-29T14:32:33.726210",
      "end_time": "2025-06-29T14:44:34.848625",
      "duration": 721.1223912239075,
      "duration_minutes": 12.018706520398458,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/data50-epoch5-rho2/log_lm_eval/vaccine_final_all_gpu1.log",
      "stdout": " super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.0/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/merged_lora_pvkwsp9d\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009358",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "vaccine_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/data50-epoch5-rho2",
      "status": "completed",
      "start_time": "2025-06-29T14:44:37.296202",
      "end_time": "2025-06-29T14:56:01.554305",
      "duration": 684.2580823898315,
      "duration_minutes": 11.40430137316386,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/data50-epoch5-rho2/log_lm_eval/vaccine_final_all_gpu1.log",
      "stdout": "__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_llama2_sys3-train_llama2_sys3-eval_llama2_sys3/merged_lora_jhv07z31\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009344",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    },
    {
      "experiment_name": "vaccine_final",
      "lora_path": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/data50-epoch5-rho2",
      "status": "completed",
      "start_time": "2025-06-29T14:44:37.260192",
      "end_time": "2025-06-29T14:56:09.680055",
      "duration": 692.4198422431946,
      "duration_minutes": 11.540330704053243,
      "log_file": "/data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/data50-epoch5-rho2/log_lm_eval/vaccine_final_all_gpu1.log",
      "stdout": " super().__init__(input_size=input_size,\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 287, in __init__\n    self.quant_method.create_weights(\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 109, in create_weights\n    weight = Parameter(torch.empty(sum(output_partition_sizes),\n  File \"/home/jin509/anaconda3/envs/malicious_finetune/lib/python3.10/site-packages/torch/utils/_device.py\", line 78, in __torch_function__\n    return func(*args, **kwargs)\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \nüßπ Ê≠•È™§6: Ê∏ÖÁêÜ‰∏¥Êó∂Êñá‰ª∂ /data3/user/jin509/malicious-finetuning/experiments-booster-vaccine-repnoise/vaccine/gsm8k-BeaverTails-p0.6/Llama-2-7b-chat-hf-lora-r64-e10-b4-data50-pre_align_alpaca-train_alpaca-eval_alpaca/merged_lora_5b03ao3y\n‚úÖ ‰∏¥Êó∂Êñá‰ª∂Ê∏ÖÁêÜÂÆåÊàê\n‚ùå Â§ö‰ªªÂä°ËØÑÊµãÊµÅÁ®ãÂ§±Ë¥•: CUDA out of memory. Tried to allocate 96.00 MiB. GPU \n",
      "stderr": "",
      "base_model": "meta-llama/Llama-2-7b-chat-hf",
      "tasks": "all",
      "worker_pid": "4009343",
      "gpu_id": "0",
      "num_gpus_used": 1,
      "visible_gpus": [
        0
      ],
      "tensor_parallel_size": 1,
      "error_message": ""
    }
  ]
}