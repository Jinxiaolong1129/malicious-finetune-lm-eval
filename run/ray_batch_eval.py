#!/usr/bin/env python3
# run/ray_batch_eval.py - æ”¯æŒåŠ¨æ€GPUæ•°é‡çš„Rayå¹¶è¡Œæ‰¹é‡LoRAæ¨¡å‹è¯„æµ‹è„šæœ¬

import os
import json
import argparse
import time
import csv
import pandas as pd
import fcntl
from pathlib import Path
from typing import List, Dict, Any, Optional
import ray
from datetime import datetime
import threading


# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ["HF_ALLOW_CODE_EVAL"] = "1"
os.environ['TOKENIZERS_PARALLELISM'] = 'false'

class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨ - ä½¿ç”¨CSVæ–‡ä»¶ç®¡ç†ä»»åŠ¡çŠ¶æ€"""
    
    def __init__(self, progress_file: str = "lm_eval_experiment_progress.csv"):
        self.progress_file = Path(progress_file)
        self.lock = threading.Lock()
        self.csv_columns = [
            'lora_path', 'experiment_name', 'status', 'start_time', 'end_time', 
            'duration_minutes', 'base_model',
            'log_file', 'error_message', 'worker_pid', 
            'gpu_id', 'retry_count', 'tasks', 'created_time',
            'num_gpus_used'
        ]
        
        self.column_dtypes = None
        
        if not self.progress_file.exists():
            self._initialize_csv()
    
    def _initialize_csv(self):
        """åˆå§‹åŒ–CSVæ–‡ä»¶"""
        try:
            self.progress_file.parent.mkdir(parents=True, exist_ok=True)
            df = pd.DataFrame(columns=self.csv_columns)
            df.to_csv(self.progress_file, index=False)
            print(f"ğŸ“Š åˆå§‹åŒ–è¿›åº¦æ–‡ä»¶: {self.progress_file}")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def load_progress(self) -> Dict[str, Dict[str, Any]]:
        """åŠ è½½ç°æœ‰è¿›åº¦ - ä¿®å¤ç±»å‹è½¬æ¢é—®é¢˜"""
        if not self.progress_file.exists():
            return {}
        
        try:
            # ä½¿ç”¨ low_memory=False é¿å…ç±»å‹æ¨æ–­è­¦å‘Š
            df = pd.read_csv(self.progress_file, low_memory=False)
            if df.empty:
                return {}
            
            progress = {}
            for _, row in df.iterrows():
                # ä½¿ç”¨lora_pathä½œä¸ºå”¯ä¸€æ ‡è¯†
                lora_path = str(row['lora_path'])
                progress[lora_path] = row.to_dict()
            
            print(f"ğŸ“Š åŠ è½½è¿›åº¦æ–‡ä»¶: {self.progress_file}")
            print(f"ğŸ“ˆ å·²è®°å½•ä»»åŠ¡æ•°: {len(progress)}")
            
            status_counts = df['status'].value_counts().to_dict()
            for status, count in status_counts.items():
                print(f"  - {status}: {count}")
            
            return progress
            
        except Exception as e:
            print(f"âš ï¸  åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
            return {}
    
    def _safe_convert_value(self, key: str, value: Any) -> Any:
        """å®‰å…¨è½¬æ¢å€¼åˆ°åˆé€‚çš„ç±»å‹ - ç®€åŒ–ç‰ˆæœ¬"""
        if value is None or pd.isna(value):
            # æ ¹æ®åˆ—ç±»å‹è¿”å›åˆé€‚çš„é»˜è®¤å€¼
            if key in ['duration_minutes']:
                return 0.0
            elif key in ['retry_count', 'num_gpus_used']:
                return 0
            else:
                return ''
        try:
            if key in ['duration_minutes'] and value != '':
                return float(value)
            elif key in ['retry_count', 'num_gpus_used'] and value != '':
                return int(float(value))  # å…ˆè½¬floatå†è½¬intï¼Œå¤„ç†"1.0"è¿™æ ·çš„æƒ…å†µ
            else:
                return str(value)
        except (ValueError, TypeError):
            if key in ['duration_minutes']:
                return 0.0
            elif key in ['retry_count', 'num_gpus_used']:
                return 0
            else:
                return str(value) if value is not None else ''
    
    def update_task_status(self, lora_path: str, **kwargs):
        """æ›´æ–°å•ä¸ªä»»åŠ¡çŠ¶æ€ - ä½¿ç”¨lora_pathä½œä¸ºå”¯ä¸€æ ‡è¯†"""
        with self.lock:
            try:
                if self.progress_file.exists():
                    df = pd.read_csv(self.progress_file, low_memory=False)
                else:
                    df = pd.DataFrame(columns=self.csv_columns)
                
                # ä½¿ç”¨lora_pathä½œä¸ºå”¯ä¸€æ ‡è¯†
                mask = df['lora_path'] == lora_path
                existing_idx = df.index[mask]
                
                if len(existing_idx) > 0:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    idx = existing_idx[0]
                    for key, value in kwargs.items():
                        if key in self.csv_columns:
                            converted_value = self._safe_convert_value(key, value)
                            df.at[idx, key] = converted_value
                else:
                    # åˆ›å»ºæ–°è®°å½•
                    new_row = {}
                    for col in self.csv_columns:
                        if col == 'lora_path':
                            new_row[col] = str(lora_path)
                        elif col == 'created_time':
                            new_row[col] = datetime.now().isoformat()
                        elif col in kwargs:
                            new_row[col] = self._safe_convert_value(col, kwargs[col])
                        else:
                            new_row[col] = self._safe_convert_value(col, None)
                    
                    new_df = pd.DataFrame([new_row])
                    df = pd.concat([df, new_df], ignore_index=True)
                
                # ä¿å­˜åˆ°æ–‡ä»¶ï¼Œä¸æŒ‡å®šæ•°æ®ç±»å‹
                df.to_csv(self.progress_file, index=False)
                
            except Exception as e:
                print(f"âš ï¸  æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥ ({lora_path}): {e}")


    def get_pending_experiments(self, all_experiments: List[Dict[str, Any]], 
                              force_rerun: bool = False,
                              retry_failed_only: bool = False) -> List[Dict[str, Any]]:
        """æ ¹æ®è¿›åº¦çŠ¶æ€ç­›é€‰å¾…æ‰§è¡Œçš„å®éªŒ"""
        if force_rerun:
            print("ğŸ”„ å¼ºåˆ¶é‡è·‘æ¨¡å¼ï¼šæ‰€æœ‰ä»»åŠ¡éƒ½å°†é‡æ–°æ‰§è¡Œ")
            return all_experiments
        
        existing_progress = self.load_progress()
        pending_experiments = []
        skipped_count = 0
        
        for exp in all_experiments:
            lora_path = str(exp['lora_path'])  # ä½¿ç”¨lora_pathä½œä¸ºå”¯ä¸€æ ‡è¯†
            exp_name = exp['experiment_name']
            current_status = existing_progress.get(lora_path, {}).get('status', 'pending')
            
            should_run = False
            
            if retry_failed_only:
                if current_status in ['failed', 'timeout', 'error']:
                    should_run = True
                    print(f"ğŸ”„ é‡è¯•å¤±è´¥ä»»åŠ¡: {exp_name} (ä¸Šæ¬¡çŠ¶æ€: {current_status})")
                elif current_status == 'completed':
                    skipped_count += 1
                    print(f"â­ï¸  è·³è¿‡å·²å®Œæˆ: {exp_name}")
                else:
                    should_run = True
                    print(f"â–¶ï¸  æ‰§è¡Œæ–°ä»»åŠ¡: {exp_name}")
            else:
                if current_status == 'completed':
                    skipped_count += 1
                    print(f"â­ï¸  è·³è¿‡å·²å®Œæˆ: {exp_name}")
                elif current_status in ['failed', 'timeout', 'error', 'running']:
                    should_run = True
                    if current_status == 'running':
                        print(f"ğŸ”„ é‡æ–°æ‰§è¡Œä¸­æ–­ä»»åŠ¡: {exp_name}")
                    else:
                        print(f"ğŸ”„ é‡è¯•å¤±è´¥ä»»åŠ¡: {exp_name} (ä¸Šæ¬¡çŠ¶æ€: {current_status})")
                else:
                    should_run = True
                    print(f"â–¶ï¸  æ‰§è¡Œæ–°ä»»åŠ¡: {exp_name}")
            
            if should_run:
                pending_experiments.append(exp)
        
        print(f"\nğŸ“Š ä»»åŠ¡ç­›é€‰ç»“æœ:")
        print(f"  - æ€»ä»»åŠ¡æ•°: {len(all_experiments)}")
        print(f"  - è·³è¿‡å·²å®Œæˆ: {skipped_count}")
        print(f"  - å¾…æ‰§è¡Œä»»åŠ¡: {len(pending_experiments)}")
        
        return pending_experiments

    def print_progress_summary(self):
        """æ‰“å°è¿›åº¦æ‘˜è¦"""
        existing_progress = self.load_progress()
        if not existing_progress:
            print("ğŸ“Š æ²¡æœ‰æ‰¾åˆ°è¿›åº¦è®°å½•")
            return
        
        status_counts = {}
        for lora_path, info in existing_progress.items():
            status = info.get('status', 'unknown')
            status_counts[status] = status_counts.get(status, 0) + 1
        
        print(f"\nğŸ“Š å½“å‰è¿›åº¦æ‘˜è¦:")
        print(f"  - æ€»ä»»åŠ¡æ•°: {len(existing_progress)}")
        for status, count in status_counts.items():
            print(f"  - {status}: {count}")
            
            
            
            
def create_worker_class(num_gpus: int):
    """åŠ¨æ€åˆ›å»ºWorkerç±»ï¼Œæ”¯æŒä¸åŒçš„GPUæ•°é‡"""
    
    @ray.remote(num_gpus=num_gpus)
    class LoRAEvaluationWorker:
        """Rayè¿œç¨‹å·¥ä½œå™¨ - æ”¯æŒåŠ¨æ€GPUæ•°é‡"""
        
        def __init__(self):
            """åˆå§‹åŒ–å·¥ä½œå™¨"""
            import subprocess
            import sys
            import torch
            self.subprocess = subprocess
            self.sys = sys
            self.num_gpus = num_gpus
            
            self.worker_pid = os.getpid()
            if torch.cuda.is_available():
                self.gpu_id = torch.cuda.current_device()
                self.gpu_count = torch.cuda.device_count()
                
                # å¦‚æœä½¿ç”¨å¤šGPUï¼Œè·å–æ‰€æœ‰å¯è§çš„GPU
                if num_gpus > 1:
                    visible_devices = os.environ.get('CUDA_VISIBLE_DEVICES', '')
                    self.visible_gpus = visible_devices.split(',') if visible_devices else list(range(self.gpu_count))
                else:
                    self.visible_gpus = [self.gpu_id]
            else:
                self.gpu_id = None
                self.gpu_count = 0
                self.visible_gpus = []
                
            print(f"ğŸ”§ Worker {self.worker_pid} åˆå§‹åŒ–å®Œæˆ")
            print(f"ğŸ¯ åˆ†é…GPUæ•°é‡: {num_gpus}")
            print(f"ğŸ¯ å½“å‰GPU ID: {self.gpu_id}")
            print(f"ğŸ¯ å¯è§GPU: {self.visible_gpus}")
                
        def evaluate_model(self, 
                           experiment_name: str,
                           base_model: str, 
                           lora_path: str,
                           tasks: str = "humaneval",
                           gpu_memory_utilization: float = 0.8,
                           tensor_parallel_size: Optional[int] = None) -> Dict[str, Any]:
            """è¯„æµ‹å•ä¸ªLoRAæ¨¡å‹ - æ”¯æŒå¤šGPU"""
            start_time = time.time()
            start_time_str = datetime.now().isoformat()
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®štensor_parallel_sizeï¼Œä½¿ç”¨num_gpus
            if tensor_parallel_size is None:
                tensor_parallel_size = self.num_gpus
            
            try:
                print(f"ğŸš€ [{experiment_name}] Worker {self.worker_pid} å¼€å§‹è¯„æµ‹")
                print(f"ğŸ“ åŸºç¡€æ¨¡å‹: {base_model}")
                print(f"ğŸ“ LoRAè·¯å¾„: {lora_path}")
                print(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {tasks}")
                print(f"ğŸ¯ ä½¿ç”¨GPUæ•°é‡: {self.num_gpus}")
                print(f"ğŸ¯ å¼ é‡å¹¶è¡Œåº¦: {tensor_parallel_size}")
                print(f"ğŸ¯ GPU IDs: {self.visible_gpus}")
                
                # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
                lora_dir = Path(lora_path)
                log_dir = lora_dir / "log_lm_eval"
                log_dir.mkdir(parents=True, exist_ok=True)
                
                log_file = log_dir / f"{experiment_name}_{tasks}_gpu{self.num_gpus}.log"
                
                # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
                cmd = [
                    self.sys.executable, "run/ray-run_evaluation.py",
                    "--base-model", base_model,
                    "--lora-path", str(lora_dir),
                    "--tasks", tasks,
                    "--output-path", str(lora_dir),
                    "--tensor-parallel-size", str(tensor_parallel_size),
                    "--gpu-memory-utilization", str(gpu_memory_utilization),
                ]

                print(f"ğŸ”„ [{experiment_name}] Worker {self.worker_pid} æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
                print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
                
                # æ‰§è¡Œè¯„æµ‹è„šæœ¬
                with open(log_file, 'w', encoding='utf-8') as log_f:
                    log_f.write(f"=== è¯„æµ‹å¼€å§‹æ—¶é—´: {start_time_str} ===\n")
                    log_f.write(f"Worker PID: {self.worker_pid}\n")
                    log_f.write(f"ä½¿ç”¨GPUæ•°é‡: {self.num_gpus}\n")
                    log_f.write(f"å¼ é‡å¹¶è¡Œåº¦: {tensor_parallel_size}\n")
                    log_f.write(f"GPU IDs: {self.visible_gpus}\n")
                    log_f.write(f"å®éªŒåç§°: {experiment_name}\n")
                    log_f.write(f"åŸºç¡€æ¨¡å‹: {base_model}\n")
                    log_f.write(f"LoRAè·¯å¾„: {lora_path}\n")
                    log_f.write(f"è¯„æµ‹ä»»åŠ¡: {tasks}\n")
                    log_f.write(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
                    log_f.write(f"{'='*80}\n\n")
                    log_f.flush()
                    
                    result = self.subprocess.run(
                        cmd,
                        stdout=log_f,
                        stderr=self.subprocess.STDOUT,
                        text=True,
                        timeout=3600  # 1å°æ—¶è¶…æ—¶
                    )
                
                # è¯»å–æ—¥å¿—æ–‡ä»¶å†…å®¹
                try:
                    with open(log_file, 'r', encoding='utf-8') as log_f:
                        log_content = log_f.read()
                        brief_output = log_content[-1000:] if len(log_content) > 1000 else log_content
                except Exception as e:
                    brief_output = f"æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶: {e}"
                
                end_time = time.time()
                end_time_str = datetime.now().isoformat()
                duration = end_time - start_time
                duration_minutes = duration / 60
                
                # åœ¨æ—¥å¿—æ–‡ä»¶æœ«å°¾æ·»åŠ ç»“æŸä¿¡æ¯
                try:
                    with open(log_file, 'a', encoding='utf-8') as log_f:
                        log_f.write(f"\n{'='*80}\n")
                        log_f.write(f"=== è¯„æµ‹ç»“æŸæ—¶é—´: {end_time_str} ===\n")
                        log_f.write(f"Worker PID: {self.worker_pid}\n")
                        log_f.write(f"è¿”å›ç : {result.returncode}\n")
                        log_f.write(f"æ€»è€—æ—¶: {duration:.2f}ç§’ ({duration_minutes:.2f}åˆ†é’Ÿ)\n")
                        log_f.write(f"çŠ¶æ€: {'æˆåŠŸ' if result.returncode == 0 else 'å¤±è´¥'}\n")
                except Exception as e:
                    print(f"âš ï¸  [{experiment_name}] Worker {self.worker_pid} å†™å…¥æ—¥å¿—ç»“æŸä¿¡æ¯å¤±è´¥: {e}")
                
                if result.returncode == 0:
                    print(f"âœ… [{experiment_name}] Worker {self.worker_pid} è¯„æµ‹æˆåŠŸå®Œæˆ")
                    print(f"â±ï¸  è€—æ—¶: {duration:.2f}ç§’ ({duration_minutes:.2f}åˆ†é’Ÿ)")
                    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
                    
                    return {
                        "experiment_name": experiment_name,
                        "lora_path": lora_path,
                        "status": "completed",
                        "start_time": start_time_str,
                        "end_time": end_time_str,
                        "duration": duration,
                        "duration_minutes": duration_minutes,
                        "log_file": str(log_file),
                        "stdout": brief_output,
                        "stderr": "",
                        "base_model": base_model,
                        "tasks": tasks,
                        "worker_pid": str(self.worker_pid),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        "gpu_id": str(self.gpu_id) if self.gpu_id is not None else "",  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        "num_gpus_used": self.num_gpus,
                        "visible_gpus": self.visible_gpus,
                        "tensor_parallel_size": tensor_parallel_size,
                        "error_message": ""
                    }
                else:
                    print(f"âŒ [{experiment_name}] Worker {self.worker_pid} è¯„æµ‹å¤±è´¥")
                    print(f"ğŸ” é”™è¯¯ä»£ç : {result.returncode}")
                    print(f"ğŸ“ è¯¦ç»†æ—¥å¿—è¯·æŸ¥çœ‹: {log_file}")
                    
                    return {
                        "experiment_name": experiment_name,
                        "lora_path": lora_path,
                        "status": "failed",
                        "start_time": start_time_str,
                        "end_time": end_time_str,
                        "duration": duration,
                        "duration_minutes": duration_minutes,
                        "error_code": result.returncode,
                        "log_file": str(log_file),
                        "stderr": "",
                        "base_model": base_model,
                        "tasks": tasks,
                        "worker_pid": str(self.worker_pid),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        "gpu_id": str(self.gpu_id) if self.gpu_id is not None else "",  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                        "num_gpus_used": self.num_gpus,
                        "visible_gpus": self.visible_gpus,
                        "tensor_parallel_size": tensor_parallel_size,
                        "error_message": f"Process failed with code {result.returncode}"
                    }
                    
            except self.subprocess.TimeoutExpired:
                end_time_str = datetime.now().isoformat()
                duration = time.time() - start_time
                duration_minutes = duration / 60
                
                print(f"â° [{experiment_name}] Worker {self.worker_pid} è¯„æµ‹è¶…æ—¶")
                
                try:
                    if 'log_file' in locals():
                        with open(log_file, 'a', encoding='utf-8') as log_f:
                            log_f.write(f"\n{'='*80}\n")
                            log_f.write(f"=== è¯„æµ‹è¶…æ—¶: {end_time_str} ===\n")
                            log_f.write(f"Worker PID: {self.worker_pid}\n")
                            log_f.write(f"è¶…æ—¶æ—¶é—´: 3600ç§’ (1å°æ—¶)\n")
                            log_f.write(f"å®é™…è¿è¡Œæ—¶é—´: {duration:.2f}ç§’ ({duration_minutes:.2f}åˆ†é’Ÿ)\n")
                except Exception:
                    pass
                
                return {
                    "experiment_name": experiment_name,
                    "lora_path": lora_path,
                    "status": "timeout",
                    "start_time": start_time_str,
                    "end_time": end_time_str,
                    "duration": duration,
                    "duration_minutes": duration_minutes,
                    "error": "Evaluation timed out after 1 hour",
                    "log_file": str(log_file) if 'log_file' in locals() else None,
                    "base_model": base_model,
                    "tasks": tasks,
                    "worker_pid": str(self.worker_pid),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "gpu_id": str(self.gpu_id) if self.gpu_id is not None else "",  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "num_gpus_used": self.num_gpus,
                    "error_message": "Evaluation timed out after 1 hour"
                }
                
            except Exception as e:
                end_time_str = datetime.now().isoformat()
                duration = time.time() - start_time
                duration_minutes = duration / 60
                
                print(f"ğŸ’¥ [{experiment_name}] Worker {self.worker_pid} è¯„æµ‹è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                
                try:
                    if 'log_file' in locals():
                        with open(log_file, 'a', encoding='utf-8') as log_f:
                            log_f.write(f"\n{'='*80}\n")
                            log_f.write(f"=== è¯„æµ‹å¼‚å¸¸: {end_time_str} ===\n")
                            log_f.write(f"Worker PID: {self.worker_pid}\n")
                            log_f.write(f"å¼‚å¸¸ä¿¡æ¯: {str(e)}\n")
                            log_f.write(f"å¼‚å¸¸å †æ ˆ:\n{traceback.format_exc()}\n")
                            log_f.write(f"è¿è¡Œæ—¶é—´: {duration:.2f}ç§’ ({duration_minutes:.2f}åˆ†é’Ÿ)\n")
                except Exception:
                    pass
                
                return {
                    "experiment_name": experiment_name,
                    "lora_path": lora_path,
                    "status": "error",
                    "start_time": start_time_str,
                    "end_time": end_time_str,
                    "duration": duration,
                    "duration_minutes": duration_minutes,
                    "error": str(e),
                    "traceback": traceback.format_exc(),
                    "log_file": str(log_file) if 'log_file' in locals() else None,
                    "base_model": base_model,
                    "tasks": tasks,
                    "worker_pid": str(self.worker_pid),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "gpu_id": str(self.gpu_id) if self.gpu_id is not None else "",  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "num_gpus_used": self.num_gpus,
                    "error_message": str(e)
                }
    
    return LoRAEvaluationWorker


class BatchEvaluationManager:
    """æ‰¹é‡è¯„æµ‹ç®¡ç†å™¨ - æ”¯æŒåŠ¨æ€GPUæ•°é‡"""
    
    def __init__(self, 
                 config_file: str,
                 tasks: str = "humaneval",
                 progress_file: str = "lm_eval_experiment_progress.csv",
                 num_gpus: int = 1,
                 tensor_parallel_size: Optional[int] = None):
        self.config_file = config_file
        self.tasks = tasks
        self.num_gpus = num_gpus
        self.tensor_parallel_size = tensor_parallel_size or num_gpus
        
        self.progress_tracker = ProgressTracker(progress_file)
        self.all_experiments = self.load_config()
        
        self.WorkerClass = create_worker_class(num_gpus)
        
    def load_config(self) -> List[Dict[str, Any]]:
        """åŠ è½½å®éªŒé…ç½®"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                experiments = json.load(f)
            
            print(f"ğŸ“ æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {self.config_file}")
            print(f"ğŸ“Š å…±æ‰¾åˆ° {len(experiments)} ä¸ªå®éªŒ")
            
            required_fields = ['experiment_name', 'base_model', 'lora_path']
            valid_experiments = []
            
            for i, exp in enumerate(experiments):
                missing_fields = [field for field in required_fields if field not in exp]
                if missing_fields:
                    print(f"âš ï¸  å®éªŒ {i} ç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}, è·³è¿‡")
                    continue
                
                if not Path(exp['lora_path']).exists():
                    print(f"âš ï¸  å®éªŒ '{exp['experiment_name']}' çš„LoRAè·¯å¾„ä¸å­˜åœ¨: {exp['lora_path']}, è·³è¿‡")
                    continue
                
                lora_dir = Path(exp['lora_path'])
                if not lora_dir.is_dir():
                    print(f"âš ï¸  å®éªŒ '{exp['experiment_name']}' çš„LoRAè·¯å¾„ä¸æ˜¯ç›®å½•: {exp['lora_path']}, è·³è¿‡")
                    continue
                
                valid_experiments.append(exp)
            
            print(f"âœ… æœ‰æ•ˆå®éªŒæ•°é‡: {len(valid_experiments)}")
            return valid_experiments
            
        except Exception as e:
            print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def run_batch_evaluation(self, 
                        force_rerun: bool = False,
                        retry_failed_only: bool = False,
                        gpu_memory_utilization: float = 0.8) -> List[Dict[str, Any]]:
        """è¿è¡Œæ‰¹é‡è¯„æµ‹"""
        if not self.all_experiments:
            print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„å®éªŒé…ç½®")
            return []
        
        pending_experiments = self.progress_tracker.get_pending_experiments(
            self.all_experiments, force_rerun, retry_failed_only
        )
        
        if not pending_experiments:
            print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆï¼")
            return []
        
        # è®¡ç®—å®é™…å¹¶å‘æ•°ï¼Œé¿å…åˆ›å»ºè¿‡å¤šç­‰å¾…çš„Actor
        import torch
        if torch.cuda.is_available():
            available_gpus = torch.cuda.device_count()
            max_concurrent = max(1, available_gpus // self.num_gpus)
            # é™åˆ¶åŒæ—¶åˆ›å»ºçš„Actoræ•°é‡ï¼Œé¿å…è¿‡å¤šç­‰å¾…
            batch_size = min(len(pending_experiments), max_concurrent + 2)  # +2ä½œä¸ºç¼“å†²
        else:
            max_concurrent = 1
            batch_size = 1
        
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡è¯„æµ‹")
        print(f"ğŸ“Š ä»»åŠ¡: {self.tasks}")
        print(f"ğŸ”¢ å¾…æ‰§è¡Œå®éªŒæ•°é‡: {len(pending_experiments)}")
        print(f"ğŸ¯ æ¯ä¸ªä»»åŠ¡ä½¿ç”¨GPUæ•°é‡: {self.num_gpus}")
        print(f"ğŸ¯ ç†è®ºæœ€å¤§å¹¶å‘: {max_concurrent}")
        print(f"ğŸ¯ é¦–æ‰¹åˆ›å»ºActoræ•°: {batch_size}")
        print(f"ğŸ¤– Rayå°†è‡ªåŠ¨ç®¡ç†GPUèµ„æºè°ƒåº¦")
        print(f"ğŸ“Š è¿›åº¦æ–‡ä»¶: {self.progress_tracker.progress_file}")
        print(f"{'='*80}")
        
        start_time = time.time()
        completed_results = []
        
        # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡åˆ›å»ºå¤ªå¤šç­‰å¾…çš„Actor
        experiment_batches = [pending_experiments[i:i+batch_size] for i in range(0, len(pending_experiments), batch_size)]
        
        for batch_idx, batch_experiments in enumerate(experiment_batches):
            print(f"\nğŸ”„ å¤„ç†ç¬¬ {batch_idx+1}/{len(experiment_batches)} æ‰¹ä»»åŠ¡ ({len(batch_experiments)} ä¸ª)")
            
            # ä¸ºå½“å‰æ‰¹æ¬¡ä»»åŠ¡åˆå§‹åŒ–è¿›åº¦çŠ¶æ€
            for exp in batch_experiments:
                self.progress_tracker.update_task_status(
                    lora_path=exp['lora_path'],  # ä½¿ç”¨lora_pathä½œä¸ºå”¯ä¸€æ ‡è¯†
                    experiment_name=exp['experiment_name'],
                    status='pending',
                    base_model=exp['base_model'],
                    tasks=self.tasks,
                    num_gpus_used=self.num_gpus,
                    retry_count=0
                )
            
            # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„Actorå’Œä»»åŠ¡
            task_info = {}  # future -> (lora_path, experiment_name, worker)
            
            for i, exp in enumerate(batch_experiments):
                # æ›´æ–°çŠ¶æ€ä¸ºrunning
                self.progress_tracker.update_task_status(
                    lora_path=exp['lora_path'],  # ä½¿ç”¨lora_pathä½œä¸ºå”¯ä¸€æ ‡è¯†
                    experiment_name=exp['experiment_name'],
                    status='running',
                    start_time=datetime.now().isoformat(),
                    num_gpus_used=self.num_gpus
                )
                
                # åˆ›å»ºWorker Actor
                worker = self.WorkerClass.remote()
                
                # æäº¤ä»»åŠ¡
                future = worker.evaluate_model.remote(
                    experiment_name=exp['experiment_name'],
                    base_model=exp['base_model'],
                    lora_path=exp['lora_path'],
                    tasks=self.tasks,
                    gpu_memory_utilization=gpu_memory_utilization,
                    tensor_parallel_size=self.tensor_parallel_size
                )
                
                # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯
                task_info[future] = {
                    'lora_path': exp['lora_path'],  # ä½¿ç”¨lora_pathä½œä¸ºå”¯ä¸€æ ‡è¯†
                    'experiment_name': exp['experiment_name'],
                    'worker': worker
                }
                
                current_task_num = batch_idx * batch_size + i + 1
                print(f"ğŸ“¤ å·²æäº¤ä»»åŠ¡ {current_task_num}/{len(pending_experiments)}: {exp['experiment_name']} (éœ€è¦ {self.num_gpus} GPU)")
            
            print(f"\nâ³ ç­‰å¾…å½“å‰æ‰¹æ¬¡ {len(task_info)} ä¸ªä»»åŠ¡å®Œæˆ...")
            if len(task_info) > max_concurrent:
                print(f"ğŸ’¡ æ³¨æ„: å½“å‰æ‰¹æ¬¡ä»»åŠ¡æ•°({len(task_info)})è¶…è¿‡å¹¶å‘æ•°({max_concurrent})ï¼Œéƒ¨åˆ†ä»»åŠ¡å°†æ’é˜Ÿç­‰å¾…")
            
            # ç­‰å¾…å½“å‰æ‰¹æ¬¡æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            remaining_futures = list(task_info.keys())
            
            while remaining_futures:
                ready_futures, remaining_futures = ray.wait(
                    remaining_futures, 
                    num_returns=1, 
                    timeout=30.0
                )
                
                for ready_future in ready_futures:
                    task_lora_path = None
                    task_name = None
                    worker = None
                    
                    if ready_future in task_info:
                        task_lora_path = task_info[ready_future]['lora_path']
                        task_name = task_info[ready_future]['experiment_name']
                        worker = task_info[ready_future]['worker']
                        del task_info[ready_future]
                    
                    try:
                        result = ray.get(ready_future)
                        completed_results.append(result)
                        
                        # æ›´æ–°è¿›åº¦æ–‡ä»¶ - ä½¿ç”¨lora_pathä½œä¸ºå”¯ä¸€æ ‡è¯†
                        self.progress_tracker.update_task_status(
                            lora_path=result['lora_path'],
                            experiment_name=result['experiment_name'],
                            status=result['status'],
                            end_time=result.get('end_time', datetime.now().isoformat()),
                            duration_minutes=result.get('duration_minutes', 0),
                            log_file=result.get('log_file', ''),
                            error_message=result.get('error_message', ''),
                            worker_pid=result.get('worker_pid'),
                            gpu_id=result.get('gpu_id'),
                            num_gpus_used=result.get('num_gpus_used', self.num_gpus)
                        )
                        
                        # æ‰“å°å®Œæˆä¿¡æ¯
                        status_emoji = "âœ…" if result['status'] == 'completed' else "âŒ"
                        duration_info = f" [{result.get('duration_minutes', 0):.1f}åˆ†é’Ÿ]"
                        gpu_info = f" [ä½¿ç”¨{result.get('num_gpus_used', self.num_gpus)}GPU]"
                        print(f"{status_emoji} {result['experiment_name']}{duration_info}{gpu_info}")
                        
                        if worker:
                            try:
                                ray.kill(worker)
                            except Exception as e:
                                print(f"âš ï¸  æ¸…ç†Workerå¤±è´¥: {e}")
                        
                    except Exception as e:
                        print(f"âŒ è·å–ä»»åŠ¡ç»“æœå¤±è´¥ ({task_name}): {e}")
                        if task_lora_path:
                            self.progress_tracker.update_task_status(
                                lora_path=task_lora_path,
                                experiment_name=task_name or 'unknown',
                                status='error',
                                end_time=datetime.now().isoformat(),
                                error_message=f"Failed to get result: {str(e)}",
                                num_gpus_used=self.num_gpus
                            )
                        
                        if worker:
                            try:
                                ray.kill(worker)
                            except Exception:
                                pass
                
                if remaining_futures:
                    completed_count = len(completed_results)
                    total_count = len(pending_experiments)
                    batch_remaining = len(remaining_futures)
                    progress_percent = completed_count / total_count * 100
                    print(f"ğŸ“ˆ æ€»è¿›åº¦: {completed_count}/{total_count} ({progress_percent:.1f}%) | å½“å‰æ‰¹æ¬¡å‰©ä½™: {batch_remaining}")
            
            # æ¸…ç†å½“å‰æ‰¹æ¬¡å‰©ä½™çš„workerå¼•ç”¨
            for future, info in task_info.items():
                try:
                    ray.kill(info['worker'])
                except Exception:
                    pass
            
            print(f"âœ… ç¬¬ {batch_idx+1} æ‰¹ä»»åŠ¡å®Œæˆ")
        
        end_time = time.time()
        total_duration = end_time - start_time
        
        # ç»Ÿè®¡ç»“æœ
        successful = [r for r in completed_results if r['status'] == 'completed']
        failed = [r for r in completed_results if r['status'] != 'completed']
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š æ‰¹é‡è¯„æµ‹å®Œæˆ!")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.2f}ç§’ ({total_duration/60:.1f}åˆ†é’Ÿ)")
        print(f"ğŸ¯ æ¯ä»»åŠ¡GPUæ•°: {self.num_gpus}")
        print(f"ğŸ¯ å¼ é‡å¹¶è¡Œåº¦: {self.tensor_parallel_size}")
        print(f"âœ… æˆåŠŸ: {len(successful)}")
        print(f"âŒ å¤±è´¥: {len(failed)}")
        if completed_results:
            print(f"ğŸ“ˆ æˆåŠŸç‡: {len(successful)/len(completed_results)*100:.1f}%")
        print(f"ğŸ“Š è¿›åº¦æ–‡ä»¶: {self.progress_tracker.progress_file}")

        # æ˜¾ç¤ºæˆåŠŸä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´
        if successful:
            print(f"\nğŸ† æˆåŠŸä»»åŠ¡æ‰§è¡Œæ—¶é—´:")
            successful_sorted = sorted(successful, key=lambda x: x.get('duration_minutes', 0))
            
            for i, result in enumerate(successful_sorted[:10], 1):
                worker_info = f"(Worker {result.get('worker_pid', 'N/A')}, {result.get('num_gpus_used', self.num_gpus)}GPU)"
                duration_info = f"[{result.get('duration_minutes', 0):.1f}min]"
                print(f"  {i:2d}. {result['experiment_name']:<25}: {duration_info} {worker_info}")
        
        # æ˜¾ç¤ºå¤±è´¥ä»»åŠ¡
        if failed:
            print(f"\nâŒ å¤±è´¥ä»»åŠ¡è¯¦æƒ…:")
            for result in failed:
                worker_info = f"Worker {result.get('worker_pid', 'N/A')}, {result.get('num_gpus_used', self.num_gpus)}GPU"
                duration_info = f"[{result.get('duration_minutes', 0):.1f}min]"
                error_msg = result.get('error_message', result.get('error', 'Unknown error'))
                print(f"  - {result['experiment_name']}: {result['status']} - {error_msg} {duration_info} ({worker_info})")
        
        # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ±‡æ€»ç›®å½•
        summary_dir = Path("./batch_evaluation_summaries")
        summary_dir.mkdir(parents=True, exist_ok=True)
        self.save_batch_results(completed_results, total_duration, summary_dir)
        
        return completed_results
    
    
    def save_batch_results(self, results: List[Dict[str, Any]], total_duration: float, summary_dir: Path):
        """ä¿å­˜æ‰¹é‡è¯„æµ‹ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_file = summary_dir / f"batch_evaluation_summary_{timestamp}.json"
        
        summary = {
            "evaluation_time": datetime.now().isoformat(),
            "config_file": str(self.config_file),
            "tasks": self.tasks,
            "num_gpus_per_task": self.num_gpus,
            "tensor_parallel_size": self.tensor_parallel_size,
            "total_experiments": len(self.all_experiments),
            "pending_experiments": len(results) if results else 0,
            "total_duration_seconds": total_duration,
            "total_duration_minutes": total_duration / 60,
            "output_mode": "lora_directories",
            "scheduling_mode": "ray_auto_scheduling",
            "progress_file": str(self.progress_tracker.progress_file),
            "results_summary": {
                "successful": len([r for r in results if r['status'] == 'completed']),
                "failed": len([r for r in results if r['status'] != 'completed']),
                "success_rate": len([r for r in results if r['status'] == 'completed']) / len(results) * 100 if results else 0
            },
            "detailed_results": results
        }
        
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç»“æœæ‘˜è¦å¤±è´¥: {e}")

def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description="æ”¯æŒåŠ¨æ€GPUæ•°é‡çš„Rayå¹¶è¡Œæ‰¹é‡LoRAæ¨¡å‹è¯„æµ‹è„šæœ¬")
    
    parser.add_argument("--config", type=str, required=True,
                        help="å®éªŒé…ç½®JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--tasks", type=str, default="humaneval",
                        help="è¯„æµ‹ä»»åŠ¡")
    parser.add_argument("--num-gpus", type=int, default=1,
                        help="æ¯ä¸ªä»»åŠ¡ä½¿ç”¨çš„GPUæ•°é‡ (é»˜è®¤: 1)")
    parser.add_argument("--tensor-parallel-size", type=int, default=None,
                        help="å¼ é‡å¹¶è¡Œå¤§å° (é»˜è®¤: ç­‰äºnum-gpus)")
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.8,
                        help="GPUå†…å­˜åˆ©ç”¨ç‡ (é»˜è®¤: 0.8)")
    parser.add_argument("--ray-address", type=str, default=None,
                        help="Rayé›†ç¾¤åœ°å€ (Noneè¡¨ç¤ºæœ¬åœ°æ¨¡å¼)")
    parser.add_argument("--progress-file", type=str, default="lm_eval_experiment_progress.csv",
                        help="è¿›åº¦è¿½è¸ªæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--force-rerun", action="store_true",
                        help="å¼ºåˆ¶é‡è·‘æ‰€æœ‰ä»»åŠ¡ï¼ˆå¿½ç•¥å·²å®ŒæˆçŠ¶æ€ï¼‰")
    parser.add_argument("--retry-failed-only", action="store_true",
                        help="åªé‡æ–°è¿è¡Œå¤±è´¥çš„ä»»åŠ¡")
    parser.add_argument("--show-progress", action="store_true",
                        help="åªæ˜¾ç¤ºå½“å‰è¿›åº¦ï¼Œä¸æ‰§è¡Œè¯„æµ‹")
    
    return parser

def validate_gpu_config(num_gpus: int, tensor_parallel_size: Optional[int] = None) -> tuple:
    """éªŒè¯GPUé…ç½®çš„åˆç†æ€§"""
    import torch
    
    if not torch.cuda.is_available():
        print("âš ï¸  è­¦å‘Š: CUDAä¸å¯ç”¨ï¼Œå°†åœ¨CPUæ¨¡å¼ä¸‹è¿è¡Œ")
        return 0, 1
    
    available_gpus = torch.cuda.device_count()
    print(f"ğŸ¯ ç³»ç»Ÿå¯ç”¨GPUæ•°é‡: {available_gpus}")
    
    if num_gpus > available_gpus:
        print(f"âš ï¸  è­¦å‘Š: è¯·æ±‚çš„GPUæ•°é‡ ({num_gpus}) è¶…è¿‡å¯ç”¨æ•°é‡ ({available_gpus})")
        print(f"ğŸ”§ è‡ªåŠ¨è°ƒæ•´ä¸ºæœ€å¤§å¯ç”¨æ•°é‡: {available_gpus}")
        num_gpus = available_gpus
    
    if tensor_parallel_size is None:
        tensor_parallel_size = num_gpus
    
    if tensor_parallel_size > num_gpus:
        print(f"âš ï¸  è­¦å‘Š: å¼ é‡å¹¶è¡Œå¤§å° ({tensor_parallel_size}) å¤§äºGPUæ•°é‡ ({num_gpus})")
        print(f"ğŸ”§ è‡ªåŠ¨è°ƒæ•´å¼ é‡å¹¶è¡Œå¤§å°ä¸º: {num_gpus}")
        tensor_parallel_size = num_gpus
    
    return num_gpus, tensor_parallel_size

def estimate_concurrent_tasks(num_gpus_per_task: int, available_gpus: int) -> int:
    """ä¼°ç®—æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°"""
    if num_gpus_per_task == 0:
        return 1  # CPUæ¨¡å¼
    
    max_concurrent = available_gpus // num_gpus_per_task
    return max(1, max_concurrent)

def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()
    
    print("ğŸš€ æ”¯æŒåŠ¨æ€GPUæ•°é‡çš„Rayå¹¶è¡Œæ‰¹é‡LoRAæ¨¡å‹è¯„æµ‹ç³»ç»Ÿ")
    print(f"ğŸ“ é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {args.tasks}")
    print(f"ğŸ¯ æ¯ä»»åŠ¡GPUæ•°é‡: {args.num_gpus}")
    print(f"ğŸ¯ å¼ é‡å¹¶è¡Œå¤§å°: {args.tensor_parallel_size or args.num_gpus}")
    print(f"ğŸ’¾ GPUå†…å­˜åˆ©ç”¨ç‡: {args.gpu_memory_utilization}")
    print(f"ğŸ“Š è¿›åº¦æ–‡ä»¶: {args.progress_file}")
    
    # éªŒè¯GPUé…ç½®
    validated_num_gpus, validated_tensor_parallel_size = validate_gpu_config(
        args.num_gpus, args.tensor_parallel_size
    )
    
    if validated_num_gpus != args.num_gpus:
        args.num_gpus = validated_num_gpus
    if validated_tensor_parallel_size != (args.tensor_parallel_size or args.num_gpus):
        args.tensor_parallel_size = validated_tensor_parallel_size
    
    # ä¼°ç®—å¹¶å‘ä»»åŠ¡æ•°
    if validated_num_gpus > 0:
        import torch
        available_gpus = torch.cuda.device_count()
        max_concurrent = estimate_concurrent_tasks(args.num_gpus, available_gpus)
        print(f"ğŸ“ˆ é¢„ä¼°æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°: {max_concurrent} (åŸºäº {available_gpus} ä¸ªGPU)")
        
        if max_concurrent == 1 and args.num_gpus > 1:
            print(f"ğŸ’¡ æç¤º: ç”±äºæ¯ä¸ªä»»åŠ¡éœ€è¦ {args.num_gpus} GPUï¼Œä»»åŠ¡å°†ä¸²è¡Œæ‰§è¡Œ")
        elif max_concurrent > 1:
            total_gpu_usage = max_concurrent * args.num_gpus
            print(f"ğŸ’¡ æç¤º: æœ€å¤š {max_concurrent} ä¸ªä»»åŠ¡å¹¶è¡Œï¼Œæ€»è®¡ä½¿ç”¨ {total_gpu_usage} GPU")
    
    if args.force_rerun:
        print(f"ğŸ”„ è¿è¡Œæ¨¡å¼: å¼ºåˆ¶é‡è·‘æ‰€æœ‰ä»»åŠ¡")
    elif args.retry_failed_only:
        print(f"ğŸ”„ è¿è¡Œæ¨¡å¼: åªé‡è·‘å¤±è´¥ä»»åŠ¡")
    else:
        print(f"ğŸ”„ è¿è¡Œæ¨¡å¼: æ™ºèƒ½ç»­ä¼  (è·³è¿‡å·²å®Œæˆ)")
    
    print(f"ğŸ¤– è°ƒåº¦æ¨¡å¼: Rayè‡ªåŠ¨èµ„æºç®¡ç†")
    print(f"ğŸ”§ ç‰¹æ€§: æ”¯æŒåŠ¨æ€GPUé…ç½®")
    
    # å¦‚æœåªæ˜¯æŸ¥çœ‹è¿›åº¦ï¼Œä¸éœ€è¦åˆå§‹åŒ–Ray
    if args.show_progress:
        print(f"\nğŸ“Š æ˜¾ç¤ºå½“å‰è¿›åº¦:")
        progress_tracker = ProgressTracker(args.progress_file)
        progress_tracker.print_progress_summary()
        return
    
    # åˆå§‹åŒ–Ray
    if args.ray_address:
        print(f"ğŸŒ è¿æ¥åˆ°Rayé›†ç¾¤: {args.ray_address}")
        ray.init(address=args.ray_address)
    else:
        print(f"ğŸ–¥ï¸  å¯åŠ¨æœ¬åœ°Rayé›†ç¾¤")
        ray.init()
    
    try:
        # åˆ›å»ºæ‰¹é‡è¯„æµ‹ç®¡ç†å™¨
        manager = BatchEvaluationManager(
            config_file=args.config,
            tasks=args.tasks,
            progress_file=args.progress_file,
            num_gpus=args.num_gpus,
            tensor_parallel_size=args.tensor_parallel_size
        )
        
        # è¿è¡Œæ‰¹é‡è¯„æµ‹
        results = manager.run_batch_evaluation(
            force_rerun=args.force_rerun,
            retry_failed_only=args.retry_failed_only,
            gpu_memory_utilization=args.gpu_memory_utilization
        )
        
        print(f"\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š è¿›åº¦æ–‡ä»¶: {args.progress_file}")
        print(f"ğŸ“„ æ±‡æ€»æ–‡ä»¶: ./batch_evaluation_summaries/")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ --show-progress å¯ä»¥éšæ—¶æŸ¥çœ‹è¿›åº¦")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ --retry-failed-only å¯ä»¥åªé‡è·‘å¤±è´¥çš„ä»»åŠ¡")
        print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ --num-gpus N å¯ä»¥æŒ‡å®šæ¯ä¸ªä»»åŠ¡ä½¿ç”¨çš„GPUæ•°é‡")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­è¯„æµ‹")
        print(f"ğŸ“Š å½“å‰è¿›åº¦å·²ä¿å­˜åˆ°: {args.progress_file}")
        print(f"ğŸ’¡ å¯ä»¥ä½¿ç”¨ç›¸åŒå‘½ä»¤é‡æ–°å¯åŠ¨ä»¥ç»§ç»­æœªå®Œæˆçš„ä»»åŠ¡")
    except Exception as e:
        print(f"ğŸ’¥ è¯„æµ‹è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # å…³é—­Ray
        try:
            ray.shutdown()
            print(f"ğŸ”„ Rayé›†ç¾¤å·²å…³é—­")
        except:
            pass

if __name__ == "__main__":
    main()