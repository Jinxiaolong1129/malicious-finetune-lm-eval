#!/usr/bin/env python3
# ray-run_evaluation.py 

import os
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ["HF_ALLOW_CODE_EVAL"] = "1"
os.environ['TOKENIZERS_PARALLELISM'] = 'false'


import argparse
import shutil
import json
import tempfile
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from lm_eval import simple_evaluate
from lm_eval.utils import (
    handle_non_serializable,
    make_table,
    simple_parse_args_string,
)
from lm_eval.loggers import EvaluationTracker
from lm_eval.utils import (
    get_file_datetime,
    get_file_task_name,
    get_results_filenames,
    get_sample_results_filenames,
    handle_non_serializable,
    hash_string,
    sanitize_list,
    sanitize_model_name,
    sanitize_task_name,
)

class OptimizedLoRAEvaluator:
    """ä¼˜åŒ–çš„LoRAæ¨¡å‹è¯„æµ‹å™¨ï¼šæ”¯æŒå¤šä»»åŠ¡ä¸€æ¬¡åŠ è½½"""
    
    def __init__(self, base_model_name, lora_path):
        self.base_model_name = base_model_name
        self.lora_path = lora_path
        self.merged_path = None
        self.results = None
        self.log_samples = True
        
    def merge_lora(self, temp_dir=None):
        """åˆå¹¶ LoRA æƒé‡åˆ°ä¸´æ—¶ç›®å½•"""
        if temp_dir is None:
            # åœ¨ lora_path ä¸‹åˆ›å»ºä¸´æ—¶ç›®å½•
            lora_parent_dir = Path(self.lora_path).parent
            temp_dir = tempfile.mkdtemp(prefix="merged_lora_", dir=str(lora_parent_dir))
        
        self.merged_path = temp_dir
        
        print(f"ğŸ”„ æ­¥éª¤1: åŠ è½½åŸºç¡€æ¨¡å‹ {self.base_model_name}")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print(f"ğŸ”„ æ­¥éª¤2: åŠ è½½ LoRA é€‚é…å™¨ {self.lora_path}")
        model = PeftModel.from_pretrained(base_model, self.lora_path)
        
        print(f"ğŸ”„ æ­¥éª¤3: åˆå¹¶ LoRA æƒé‡")
        merged_model = model.merge_and_unload()
        
        print(f"ğŸ”„ æ­¥éª¤4: ä¿å­˜åˆ°ä¸´æ—¶ç›®å½• {self.merged_path}")
        os.makedirs(self.merged_path, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        merged_model.save_pretrained(
            self.merged_path,
            safe_serialization=True,
            max_shard_size="2GB"
        )
        
        # ä¿å­˜ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        tokenizer.save_pretrained(self.merged_path)
        
        # æ¸…ç†å†…å­˜
        del base_model, model, merged_model, tokenizer
        torch.cuda.empty_cache()
        
        print(f"âœ… æ¨¡å‹åˆå¹¶å®Œæˆï¼Œä¸´æ—¶ä¿å­˜åœ¨: {self.merged_path}")
        return self.merged_path
    
    def evaluate_multiple_tasks(self, 
                               tasks=["humaneval"], 
                               tensor_parallel_size=1, 
                               gpu_memory_utilization=0.8,
                               **eval_kwargs):
        """ä½¿ç”¨ vLLM è¯„æµ‹å¤šä¸ªä»»åŠ¡ - ä¿®æ­£ç‰ˆ + GSM8K + ARC Challenge æ”¯æŒ"""
        if not self.merged_path or not os.path.exists(self.merged_path):
            raise ValueError("è¯·å…ˆè°ƒç”¨ merge_lora() åˆå¹¶æ¨¡å‹")
        
        print(f"\nğŸš€ æ­¥éª¤5: ä½¿ç”¨ vLLM å¼€å§‹å¤šä»»åŠ¡è¯„æµ‹...")
        print(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {', '.join(tasks)} (å…±{len(tasks)}ä¸ª)")
        print(f"âš¡ Tensor Parallel Size: {tensor_parallel_size}")
        print(f"ğŸ§  GPU Memory Utilization: {gpu_memory_utilization}")
        print(f"ğŸ’¡ ä¼˜åŠ¿: æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼Œè¯„æµ‹{len(tasks)}ä¸ªä»»åŠ¡")
        
        # ä»»åŠ¡ç‰¹å®šçš„é»˜è®¤å‚æ•° 
        task_defaults = {
            "mmlu": {"num_fewshot": 0, "batch_size": "auto"},
            "humaneval": {"num_fewshot": 0, "batch_size": "auto"},  
            "gsm8k": {"num_fewshot": 0, "batch_size": "auto"},
            "arc_challenge": {"num_fewshot": None, "batch_size": "auto"},  # ä½¿ç”¨é»˜è®¤å€¼
            "truthfulqa_mc1": {"num_fewshot": 0, "batch_size": "auto"},
            "truthfulqa_mc2": {"num_fewshot": 0, "batch_size": "auto"},
        }
        
        # æ ‡å‡†åŒ–ä»»åŠ¡åç§°
        normalized_tasks = []
        has_unsafe_tasks = False
        
        for task in tasks:
            if task.lower() == "mmlu":
                normalized_tasks.append("mmlu")
            elif task.lower() == "humaneval":
                normalized_tasks.append("humaneval")
                has_unsafe_tasks = True
            elif task.lower() == "gsm8k":
                normalized_tasks.append("gsm8k")
            elif task.lower() == "arc_challenge":
                normalized_tasks.append("arc_challenge")
            elif task.lower() in ["truthfulqa", "truthfulqa_mc"]:
                # TruthfulQA é»˜è®¤è¯„æµ‹ MC1 å’Œ MC2
                normalized_tasks.extend(["truthfulqa_mc1", "truthfulqa_mc2"])
            elif task.lower() == "truthfulqa_mc1":
                normalized_tasks.append("truthfulqa_mc1")
            elif task.lower() == "truthfulqa_mc2":
                normalized_tasks.append("truthfulqa_mc2")
            else:
                normalized_tasks.append(task)
                if any(unsafe_keyword in task.lower() for unsafe_keyword in ["code", "eval", "exec"]):
                    has_unsafe_tasks = True
        
        print(f"ğŸ“‹ æ ‡å‡†åŒ–åçš„ä»»åŠ¡: {', '.join(normalized_tasks)}")
        
        if has_unsafe_tasks:
            print(f"âš ï¸  æ£€æµ‹åˆ°åŒ…å«ä»£ç æ‰§è¡Œä»»åŠ¡ (å¦‚ HumanEval)ï¼Œå°†è‡ªåŠ¨å¯ç”¨å®‰å…¨ç¡®è®¤å‚æ•°")
        
        # æŒ‰ num_fewshot åˆ†ç»„è¯„æµ‹ - æ”¯æŒ None å€¼
        fewshot_groups = {}
        for task in normalized_tasks:
            fewshot = task_defaults.get(task, {}).get("num_fewshot", 0)
            if fewshot not in fewshot_groups:
                fewshot_groups[fewshot] = []
            fewshot_groups[fewshot].append(task)
        
        print(f"ğŸ¯ æŒ‰ few-shot åˆ†ç»„:")
        for fewshot, group_tasks in fewshot_groups.items():
            fewshot_display = "é»˜è®¤å€¼" if fewshot is None else f"{fewshot}-shot"
            print(f"   {fewshot_display}: {', '.join(group_tasks)}")
        
        # å¦‚æœåªæœ‰ä¸€ç»„ï¼Œç›´æ¥è¯„æµ‹
        if len(fewshot_groups) == 1:
            fewshot_value = list(fewshot_groups.keys())[0]
            return self._evaluate_single_group(normalized_tasks, fewshot_value, 
                                             tensor_parallel_size, gpu_memory_utilization, 
                                             has_unsafe_tasks, **eval_kwargs)
        else:
            # å¤šç»„åˆ†åˆ«è¯„æµ‹ååˆå¹¶ç»“æœ
            return self._evaluate_multiple_groups(fewshot_groups, 
                                                tensor_parallel_size, gpu_memory_utilization,
                                                has_unsafe_tasks, **eval_kwargs)

    def _evaluate_single_group(self, tasks, num_fewshot, tensor_parallel_size, 
                              gpu_memory_utilization, has_unsafe_tasks, **eval_kwargs):
        """è¯„æµ‹å•ä¸ª few-shot ç»„"""
        model_args = {
            "pretrained": self.merged_path,
            "tensor_parallel_size": tensor_parallel_size,
            "gpu_memory_utilization": gpu_memory_utilization,
            "max_num_seqs": 256,            
            "max_num_batched_tokens": 4096, 
        }
        
        eval_args = {
            "model": "vllm",
            "model_args": model_args,
            "tasks": tasks,
            "batch_size": "auto",
            "log_samples": True
        }
        
        # åªæœ‰å½“ num_fewshot ä¸ä¸º None æ—¶æ‰ä¼ å…¥å‚æ•°
        if num_fewshot is not None:
            eval_args["num_fewshot"] = num_fewshot
            print(f"ğŸ¯ è®¾ç½® num_fewshot={num_fewshot}")
        else:
            print(f"ğŸ¯ ä½¿ç”¨ä»»åŠ¡é»˜è®¤çš„ num_fewshot å€¼")
        
        # GSM8K ç‰¹æ®Šé…ç½®
        if any(task == "gsm8k" for task in tasks):
            print(f"ğŸ§® GSM8K ç‰¹æ®Šé…ç½®ï¼šå¯ç”¨æ•°å­¦æ¨ç†ä¼˜åŒ–")
            eval_args["limit"] = None  # è¯„æµ‹å…¨éƒ¨æ ·æœ¬
        
        # ARC Challenge ç‰¹æ®Šé…ç½®
        if any(task == "arc_challenge" for task in tasks):
            print(f"ğŸ† ARC Challenge ç‰¹æ®Šé…ç½®ï¼šä½¿ç”¨é»˜è®¤ few-shot è®¾ç½®")
        
        if has_unsafe_tasks:
            eval_args["confirm_run_unsafe_code"] = True
            print(f"ğŸ” å®‰å…¨è®¾ç½®ï¼šå·²å¯ç”¨ confirm_run_unsafe_code=True")
        
        # åˆå¹¶ç”¨æˆ·å‚æ•°
        for key, value in eval_kwargs.items():
            if key not in ['evaluation_script', 'python_executable']:
                eval_args[key] = value
        
        fewshot_display = "é»˜è®¤å€¼" if num_fewshot is None else f"num_fewshot={num_fewshot}"
        print(f"â³ å¼€å§‹è¯„æµ‹ {len(tasks)} ä¸ªä»»åŠ¡ ({fewshot_display})...")
        results = simple_evaluate(**eval_args)
        print(f"âœ… ä»»åŠ¡ç»„è¯„æµ‹å®Œæˆ!")
        
        self.results = results
        return results

    def _evaluate_multiple_groups(self, fewshot_groups, tensor_parallel_size, 
                                 gpu_memory_utilization, has_unsafe_tasks, **eval_kwargs):
        """è¯„æµ‹å¤šä¸ª few-shot ç»„å¹¶åˆå¹¶ç»“æœ"""
        all_results = {"results": {}, "samples": {}}
        
        for fewshot, group_tasks in fewshot_groups.items():
            fewshot_display = "é»˜è®¤å€¼" if fewshot is None else f"{fewshot}-shot"
            print(f"\nğŸ¯ è¯„æµ‹ {fewshot_display} ç»„: {', '.join(group_tasks)}")
            
            group_results = self._evaluate_single_group(
                group_tasks, fewshot, tensor_parallel_size, 
                gpu_memory_utilization, has_unsafe_tasks, **eval_kwargs
            )
            
            # åˆå¹¶ç»“æœ
            if "results" in group_results:
                all_results["results"].update(group_results["results"])
            if "samples" in group_results:
                all_results["samples"].update(group_results["samples"])
        
        # å¤åˆ¶å…¶ä»–å…ƒæ•°æ®
        for key, value in group_results.items():
            if key not in ["results", "samples"]:
                all_results[key] = value
        
        self.results = all_results
        return all_results
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if self.merged_path and os.path.exists(self.merged_path):
            print(f"ğŸ§¹ æ­¥éª¤6: æ¸…ç†ä¸´æ—¶æ–‡ä»¶ {self.merged_path}")
            try:
                shutil.rmtree(self.merged_path)
                print("âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        else:
            print("â„¹ï¸  æ²¡æœ‰éœ€è¦æ¸…ç†çš„ä¸´æ—¶æ–‡ä»¶")
    
    def save_results(self, output_path):
        """ä¿å­˜æ¯ä¸ªä»»åŠ¡çš„ç»“æœåˆ°å•ç‹¬çš„æ–‡ä»¶"""
        if self.results is None:
            print("âš ï¸  æ²¡æœ‰è¯„æµ‹ç»“æœå¯ä¿å­˜")
            return
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = Path(output_path)
            output_dir.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ’¾ ä¿å­˜ç»“æœåˆ°ç›®å½•: {output_dir}")
            
            # å¤„ç† samples æ•°æ®
            samples = None
            results_copy = self.results.copy()
            if self.log_samples and "samples" in results_copy:
                samples = results_copy.pop("samples")
            
            # æ·»åŠ æ—¶é—´æˆ³å’Œå…ƒä¿¡æ¯
            from datetime import datetime
            date_id = datetime.now().isoformat().replace(":", "-")
            
            # è·å–è¯„æµ‹ç»“æœ
            task_results = results_copy.get("results", {})
            
            # ä¸ºæ¯ä¸ªä»»åŠ¡ä¿å­˜å•ç‹¬çš„æ–‡ä»¶
            for task_name, task_result in task_results.items():
                print(f"ğŸ’¾ ä¿å­˜ä»»åŠ¡ {task_name} çš„ç»“æœ...")
                
                # è®¡ç®—ä»»åŠ¡å“ˆå¸Œå€¼
                task_hash = ""
                if samples and task_name in samples:
                    task_samples = samples[task_name]
                    sample_hashes = [
                        s["doc_hash"] + s["prompt_hash"] + s["target_hash"]
                        for s in task_samples
                    ]
                    task_hash = hash_string("".join(sample_hashes))
                
                # æ„å»ºå•ä¸ªä»»åŠ¡çš„ç»“æœå­—å…¸
                single_task_result = {
                    "results": {task_name: task_result},
                    "task_hashes": {task_name: task_hash},
                    "evaluation_time": date_id,
                    "evaluation_mode": "multi_task_single_load",
                    "task_name": task_name,
                    "lora_path": self.lora_path,
                    "base_model": self.base_model_name
                }
                
                # å¤åˆ¶å…¶ä»–å…ƒæ•°æ®ï¼ˆæ’é™¤resultså’Œsamplesï¼‰
                for key, value in results_copy.items():
                    if key not in ["results", "task_hashes"]:
                        single_task_result[key] = value
                
                # ä¿å­˜ä¸»è¦ç»“æœæ–‡ä»¶
                results_file = output_dir / f"lm_eval_{task_name}_results.json"
                dumped = json.dumps(
                    single_task_result, 
                    indent=2, 
                    default=handle_non_serializable, 
                    ensure_ascii=False
                )
                
                with open(results_file, "w", encoding="utf-8") as f:
                    f.write(dumped)
                print(f"âœ… {task_name} ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
                
                # ä¿å­˜æ ·æœ¬æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if samples and task_name in samples:
                    samples_file = output_dir / f"lm_eval_{task_name}_results_samples.json"
                    task_samples = {task_name: samples[task_name]}
                    samples_dumped = json.dumps(
                        task_samples, 
                        indent=2, 
                        default=handle_non_serializable, 
                        ensure_ascii=False
                    )
                    with open(samples_file, "w", encoding="utf-8") as f:
                        f.write(samples_dumped)
                    print(f"âœ… {task_name} æ ·æœ¬æ•°æ®å·²ä¿å­˜åˆ°: {samples_file}")
            
            print(f"âœ… æ‰€æœ‰ä»»åŠ¡ç»“æœå·²ä¿å­˜å®Œæ¯•")
            
            # æ‰“å°ç»“æœè¡¨æ ¼
            print(f"\nğŸ“Š è¯¦ç»†è¯„æµ‹ç»“æœ:")
            print(make_table(self.results))
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            

    def run_full_pipeline(self, tasks=["mmlu"], output_path=None, **eval_kwargs):
        """è¿è¡Œå®Œæ•´çš„å¤šä»»åŠ¡è¯„æµ‹æµç¨‹ï¼šåˆå¹¶-è¯„æµ‹-æ¸…ç†"""
        try:
            print(f"ğŸš€ å¯åŠ¨å¤šä»»åŠ¡è¯„æµ‹æµç¨‹")
            print(f"ğŸ“Š ä»»åŠ¡åˆ—è¡¨: {', '.join(tasks)} (å…±{len(tasks)}ä¸ª)")
            
            # æ­¥éª¤1-4: åˆå¹¶
            self.merge_lora()
            
            # æ­¥éª¤5: å¤šä»»åŠ¡è¯„æµ‹
            self.evaluate_multiple_tasks(tasks=tasks, **eval_kwargs)
            
            # ä¿å­˜ç»“æœ
            if output_path:
                self.save_results(output_path)
            
            return self.results
            
        finally:
            # æ­¥éª¤6: æ¸…ç†ï¼ˆæ— è®ºæ˜¯å¦å‡ºé”™éƒ½ä¼šæ‰§è¡Œï¼‰
            self.cleanup()

def parse_tasks(tasks_str):
    """è§£æä»»åŠ¡å­—ç¬¦ä¸² - ä¿®æ­£ç‰ˆ + GSM8K + ARC Challenge"""
    if not tasks_str:
        return ["mmlu"]
    
    tasks = [task.strip() for task in tasks_str.split(",")]
    normalized_tasks = []
    
    for task in tasks:
        if task.lower() == "all":
            # BUG (ä¿ç•™mmluç”¨äºdebug)
            normalized_tasks.extend(["mmlu", "humaneval", "gsm8k", "arc_challenge", "truthfulqa_mc1", "truthfulqa_mc2"])
            # normalized_tasks.extend(["humaneval", "gsm8k", "arc_challenge", "truthfulqa_mc1", "truthfulqa_mc2"])
        elif task.lower() == "truthful":  # ç®€åŒ–è¾“å…¥
            normalized_tasks.extend(["truthfulqa_mc1", "truthfulqa_mc2"])
        elif task.lower() == "gsm8k":
            normalized_tasks.append("gsm8k")
        elif task.lower() == "arc_challenge":
            normalized_tasks.append("arc_challenge")
        else:
            normalized_tasks.append(task)
    
    return normalized_tasks

def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç‰ˆLoRAæ¨¡å‹è¯„æµ‹è„šæœ¬ - æ”¯æŒå¤šä»»åŠ¡ä¸€æ¬¡åŠ è½½ + GSM8K + ARC Challenge")
    
    parser.add_argument("--base-model", type=str, required=True,
                        help="åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--lora-path", type=str, required=True,
                        help="LoRAæ¨¡å‹è·¯å¾„")
    parser.add_argument("--tasks", type=str, default="humaneval",
                        help="è¯„æµ‹ä»»åŠ¡ï¼Œæ”¯æŒé€—å·åˆ†éš”å¤šä¸ªä»»åŠ¡ï¼Œå¦‚: mmlu,humaneval,gsm8k,arc_challenge,truthfulqa")
    parser.add_argument("--output-path", type=str, required=True,
                        help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--tensor-parallel-size", type=int, default=1,
                        help="vLLM tensor parallel size")
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.7,
                        help="GPUå†…å­˜ä½¿ç”¨ç‡")
    parser.add_argument("--batch-size", type=str, default="auto",
                        help="æ‰¹å¤„ç†å¤§å° (æ¨èä½¿ç”¨auto)")
    
    return parser

def main():
    """ä¸»å‡½æ•° - æ”¯æŒå¤šä»»åŠ¡å‘½ä»¤è¡Œå‚æ•° + GSM8K + ARC Challenge"""
    parser = create_parser()
    args = parser.parse_args()
    
    # å¤„ç†ä»»åŠ¡å‚æ•°
    tasks = parse_tasks(args.tasks)
    
    print("ğŸ¯ å¯åŠ¨ä¼˜åŒ–ç‰ˆ LoRA æ¨¡å‹å¤šä»»åŠ¡è¯„æµ‹æµç¨‹ (åŒ…å« GSM8K + ARC Challenge)")
    print(f"ğŸ“ åŸºç¡€æ¨¡å‹: {args.base_model}")
    print(f"ğŸ“ LoRA è·¯å¾„: {args.lora_path}")
    print(f"ğŸ“Š è¯„æµ‹ä»»åŠ¡: {', '.join(tasks)} (å…±{len(tasks)}ä¸ª)")
    print(f"âš¡ Tensor Parallel: {args.tensor_parallel_size}")
    print(f"ğŸ§  GPUå†…å­˜ä½¿ç”¨ç‡: {args.gpu_memory_utilization}")
    print(f"ğŸ’¾ è¾“å‡ºç›®å½•: {args.output_path}")
    print(f"ğŸ’¡ ä¼˜åŒ–æ¨¡å¼: ä¸€æ¬¡åŠ è½½è¯„æµ‹{len(tasks)}ä¸ªä»»åŠ¡")
    
    # ç‰¹åˆ«æç¤ºä»»åŠ¡
    if "gsm8k" in tasks:
        print(f"ğŸ§® åŒ…å«æ•°å­¦æ¨ç†ä»»åŠ¡ GSM8K (8k æ•°å­¦åº”ç”¨é¢˜)")
    if "arc_challenge" in tasks:
        print(f"ğŸ† åŒ…å«ç§‘å­¦æ¨ç†ä»»åŠ¡ ARC Challenge (ä½¿ç”¨é»˜è®¤ few-shot è®¾ç½®)")
    
    print(f"{'='*80}")
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆè¯„æµ‹å™¨
    evaluator = OptimizedLoRAEvaluator(args.base_model, args.lora_path)
    
    # æ„å»ºè¯„æµ‹å‚æ•°
    eval_kwargs = {
        "tensor_parallel_size": args.tensor_parallel_size,
        "gpu_memory_utilization": args.gpu_memory_utilization,
    }
    
    if args.batch_size != "auto":
        eval_kwargs["batch_size"] = args.batch_size
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    try:
        results = evaluator.run_full_pipeline(
            tasks=tasks,
            output_path=args.output_path,
            **eval_kwargs
        )
        
        print(f"\nğŸ‰ å¤šä»»åŠ¡è¯„æµ‹æµç¨‹å®Œæˆï¼")
        print(f"âœ… æˆåŠŸè¯„æµ‹äº† {len(tasks)} ä¸ªä»»åŠ¡")
        if "gsm8k" in tasks:
            print(f"ğŸ§® GSM8K æ•°å­¦æ¨ç†è¯„æµ‹å·²å®Œæˆ")
        if "arc_challenge" in tasks:
            print(f"ğŸ† ARC Challenge ç§‘å­¦æ¨ç†è¯„æµ‹å·²å®Œæˆ")
        print(f"âš¡ æ•ˆç‡æå‡: ç›¸æ¯”å•ä»»åŠ¡æ¨¡å¼å¿«çº¦ {len(tasks)} å€")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {args.output_path}")
        return results
        
    except Exception as e:
        print(f"âŒ å¤šä»»åŠ¡è¯„æµ‹æµç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()